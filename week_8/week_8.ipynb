{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6212bdf68f0b5a30\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6212bdf68f0b5a30\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You need to run this cell for the code in following cells to work.\n",
    "\"\"\"\n",
    "\n",
    "# Enable module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Enable interactive plots\n",
    "%matplotlib notebook\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --bind_all\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import week_8.backstage.plots as plots\n",
    "import week_8.backstage.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8\n",
    "\n",
    "__Goals for this week__\n",
    "\n",
    "We will discuss recurrent neural networks in this lab. They are useful for processing sequence data, such as sentences, time series, etc. We will also talk about word representations for natural language processing.\n",
    "\n",
    "__Feedback__\n",
    "\n",
    "This lab is a work in progress. If you notice a mistake, notify us or you can even make a pull request. Also please fill the [questionnaire](https://forms.gle/r27nBAvnMC7jbjJ58) after you finish this lab to give us feedback.\n",
    "\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "_Recurrent neural networks_ (RNN) are the last major neural architecture we will talk about during our labs. They are used to process sequence data. One-dimensional CNNs can also be used for sequence processing, however, RNNs should be better at modeling long-term dependencies between individual inputs. RNNs are also more versatile for sequence data, e.g. they can be used for tasks that expect a sequence as an output, or that expect a separate label for each input.\n",
    "\n",
    "_Recurrent cell_ lies at the heart of RNNs. Cell is the basic operation that is done as we process one step from a series of $N$ inputs $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N$. For step $i$ the cell looks like this:\n",
    "\n",
    "<img src=\"images/cell.svg\" alt=\"Recurrent cell\" style=\"width: 20%;\"/>\n",
    "   \n",
    "- $\\mathbf{x}_i$ is $i$-th input\n",
    "- $\\mathbf{y}_i$ is $i$-th output\n",
    "- $\\mathbf{s}_i$ is the state of cell for $i$-th step\n",
    "   \n",
    "All of these quantities are vectors. As the figure above illustrates, at each step the cell depends on two inputs - the input for the step itself and the state of the cell from previous step. Because the cell \"sees\" the state from previous steps, the layer can process the current step while using the knowledge about all the previous steps.\n",
    "\n",
    "We can imagine the recurrent layer as a series of cell operations:\n",
    "\n",
    "<img src=\"images/rnn.svg\" alt=\"Recurrent layer\" style=\"width: 50%;\"/>\n",
    "   \n",
    "Note that when we follow the flow of computation leading to output $\\mathbf{y}_i$, we can see that it depends on all the previous inputs $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_i$. It also depends on the initial state $\\mathbf{s}_0$, which is usually a trainable parameter vector of the model.\n",
    "\n",
    "### Recurrent cell variants\n",
    "\n",
    "Multiple variants of recurrent cells exist, e.g. this is the definition of _Ellman cell:_\n",
    "\n",
    "$\n",
    "\\mathbf{h}_i = \\sigma(\\mathbf{W}_{in}\\mathbf{x}_i + \\mathbf{W}_{hid}\\mathbf{s}_{i-1} + \\mathbf{b}_{hid}) \\\\\n",
    "\\mathbf{y}_i = \\sigma(\\mathbf{W}_{out}\\mathbf{h}_i + \\mathbf{b}_{out})  \\\\\n",
    "\\mathbf{s}_i = \\mathbf{h}_i \\\\\n",
    "$\n",
    "\n",
    "The definition of $\\mathbf{h}_i$ and $\\mathbf{y}_i$ is very similar to MLP, the only difference is that $\\mathbf{h}_i$ also depends on the state from previous step cell. In this case the state $\\mathbf{s}_i$ is simply the value of hidden layer $\\mathbf{h}_i$ within the cell. Note that the same parameters (weights and biases) are used for each step. The computation done by a cell is the same for each step, only the inputs of the cell ($\\mathbf{x}_i$ and $\\mathbf{s}_{i-1}$) differ.\n",
    "\n",
    "Simple cells like these do not perform very well. The information is being transformed by matrix multiplication each time step. This tends to dilute the information and the network \"forgets\" about what it has seen in the past. This limits the use of simple recurrent cells only for relatively short sequences. Simple cells are also quite unstable to train and suffer from so called _exploding / vanishing gradient_ problem.\n",
    "\n",
    "Instead of these simple cells we usually use more complex cells that were developed to address the issues we mentioned. Most common of these cells are _LSTM_ and _GRU_ cells. Check the further reading section if you are interested in why they tend to work better than vanilla recurrent cells or how do they look like.\n",
    "\n",
    "### Training\n",
    "\n",
    "The operations used for RNN are very similar to MLP operations. We use matrix multiplication, addition, activation functions and that is basically all there is to it. The training routine is therefore also quite similar to MLP. Again, so in previous architectures, we use _stochastic gradient descent_ to calculate the derivatives of the loss function w.r.t. each parameter.\n",
    "\n",
    "### Recurrent architectures\n",
    "\n",
    "There are multiple ways of using recurrent layers depending on the nature of the task we want to solve. In all the following examples the recurrent layer is the same, we only work differently with the inputs and outputs of this layer to get it to do what we want.\n",
    "\n",
    "#### Many to one\n",
    "\n",
    "<img src=\"images/manyone.svg\" alt=\"Many to one\" style=\"width: 40%;\"/>\n",
    "\n",
    "We feed the recurrent layer until we process the whole input. Then we use the result of this pass to get a single result. We use this type of RNN to do:\n",
    "\n",
    "- Sequence classification - We want to assign a label to a sequence (i.e. text classification, event detection).\n",
    "- Prediction - We want to predict following values in a time series.\n",
    "\n",
    "During the computation we can either:\n",
    "\n",
    "- Discard all the outputs, but the last $\\mathbf{y}_N$. Then we use only this output.\n",
    "- Pool all the outputs using mean-pooling (or max-pooling) of all the outputs $\\mathbf{y}_i$.\n",
    "\n",
    "#### One to many\n",
    "\n",
    "<img src=\"images/onemany.svg\" alt=\"One to many\" style=\"width: 40%;\"/>\n",
    "\n",
    "We feed the recurrent layer with one value and we expect it to produce multiple values. We use this type for:\n",
    "\n",
    "- Generation tasks - We want to generate a series of values based on a prompt (e.g. image captioning, music generation).\n",
    "\n",
    "During all the steps the cell expects an input $\\mathbf{x}_{i>1}$, that is how it is defined. We can either use the same input each step, or we can feed the layer with the output from previous step $\\mathbf{y}_{i-1}$.\n",
    "\n",
    "#### Many to many\n",
    "\n",
    "<img src=\"images/manymany.svg\" alt=\"Many to many\" style=\"width: 40%;\"/>\n",
    "\n",
    "We feed the recurrent layer with multiple values and we expect an output for each of them. We use this type for:\n",
    "\n",
    "- Input tagging - We want to assign each input into a class (e.g. part-of-speech tagging, event scope detection).\n",
    "\n",
    "#### Sequence to sequence\n",
    "\n",
    "<img src=\"images/seqseq.svg\" alt=\"Sequence to sequence\" style=\"width: 70%;\"/>\n",
    "\n",
    "We feed the recurrent layer a series of inputs and we expect a series as an output. We use thys type for:\n",
    "\n",
    "- Multi-hop prediction - We want to generate multiple values as a prediction.\n",
    "\n",
    "### Advanced recurrent architectures\n",
    "\n",
    "The architecture mentioned above show how can a single recurrent layer be used. In this section we show some examples of how to combine multiple layers for various use-cases.\n",
    "\n",
    "#### Bi-directional recurrent network\n",
    "\n",
    "<img src=\"images/bidirectional.svg\" alt=\"Bi-directional\" style=\"width: 70%;\"/>\n",
    "\n",
    "We can combine two recurrent layers, one that processes the data from start to end, while the other goes backwards from end to start. We simply combine the outputs of these two networks for each time step. The advantage of this combination is that for each time step the following layer \"sees\" all the inputs, not only the previous ones. Follow the flow of computation going into any output $\\mathbf{y}_i$. You will find out that it connects to all the inputs.\n",
    "\n",
    "#### Multilayer recurrent network\n",
    "\n",
    "<img src=\"images/multilayer.svg\" alt=\"Multilayer\" style=\"width: 60%;\"/>\n",
    "\n",
    "We can also simply stack multiple recurrent layers on top of each other. This is mainly used to increase the capacity of the model, i.e. its ability to model data. Usually RNNs are not as deep as CNNs and we use up to 5 layers. One layer is usually enough as a starting point.\n",
    "\n",
    "#### Hierarchical recurrent network\n",
    "\n",
    "<img src=\"images/hierarchical.svg\" alt=\"Hierarchical\" style=\"width: 70%;\"/>\n",
    "\n",
    "For sequences of sequences (e.g. sentence is a sequence of words and words are sequences of characters) hierarchical recurrent networks can be used. We again combine two networks. In the sentence-word-character case, the first processes the words character by character. The outputs of this network for each word are then fed to another RNN.\n",
    "\n",
    "#### Encoder-decoder architecture\n",
    "\n",
    "<img src=\"images/encoderdecoder.svg\" alt=\"Encoder-decoder\" style=\"width: 80%;\"/>\n",
    "\n",
    "We can combine two recurrent layers for sequence to sequence tasks as well. We then have one layer that encodes the input into a representation and the other that decodes this representation into a series of outputs. The main use case for this architecture is machine translation.\n",
    "\n",
    "### Toy example\n",
    "\n",
    "We will use recurrent neural networks for _sine_ prediction. We can easily generate our own data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_targets = data.sine_dataset(\n",
    "    num_samples=1000,  # How many sample in a dataset\n",
    "    time_steps=250,   # How long is each sample\n",
    "    skip=10)          # How far is the target for prediction\n",
    "\n",
    "test_dataset = data.sine_dataset(\n",
    "    num_samples=100,  # How many sample in a dataset\n",
    "    time_steps=250,   # How long is each sample\n",
    "    skip=10)          # How far is the target for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see how the data (4 samples) look like. The line is the sample input - each sample is a list of values. The dot in the graphs are the scalar targets. We want to predict the value of the target from the input data.\n",
    "\n",
    "__Exercise 8.1:__ Which RNN architecture would you use for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        event.shiftKey = false;\n",
       "        // Send a \"J\" for go to next cell\n",
       "        event.which = 74;\n",
       "        event.keyCode = 74;\n",
       "        manager.command_mode();\n",
       "        manager.handle_keydown(event);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"900\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots.show_sines(train_data, train_targets, skip=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily implement RNN for single-step prediction in `keras`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 11s 11ms/sample - loss: 0.3506 - val_loss: 0.2020\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 8s 8ms/sample - loss: 0.2598 - val_loss: 0.1670\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.1779 - val_loss: 0.1329\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.1063 - val_loss: 0.0590\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.0649 - val_loss: 0.0489\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.0494 - val_loss: 0.0304\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.0414 - val_loss: 0.0251\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.0330 - val_loss: 0.0330\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.0301 - val_loss: 0.0185\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 7s 7ms/sample - loss: 0.0228 - val_loss: 0.0126\n",
      "Epoch 11/50\n",
      " 130/1000 [==>...........................] - ETA: 6s - loss: 0.0134"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-29efc3548b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     validation_data=test_dataset)\n\u001b[0m",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "class RecurrentModel(keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RecurrentModel, self).__init__()\n",
    "        self.lstm = LSTM(\n",
    "            units=20)  # LSTM output shape\n",
    "        self.dense = Dense(\n",
    "            units=1)  # We want to output one number as a prediction\n",
    "        \n",
    "    def call(self, x):     # (batch_size, time_steps, input_dim)\n",
    "        x = self.lstm(x)   # (batch_suze, lstm_dim)\n",
    "        x = self.dense(x)  # (batch_size, dense_dim)\n",
    "        return x\n",
    "    \n",
    "model = RecurrentModel()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error')  # MSE is a good loss for regression tasks like this\n",
    "\n",
    "model.fit(\n",
    "    x=train_data,\n",
    "    y=train_targets,\n",
    "    epochs=50,\n",
    "    batch_size=10,\n",
    "    validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 8.2:__ Shapes of `x` are described in the `call` function of `RecurrentModel`. What are the actual values of the described quantities? E.g. what is the value of `batch_suze` in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs for Natural Language Processing\n",
    "\n",
    "RNNs are often used for _natural language processing_ (NLP). Words are sequences of letters, sentences are sequences of words, documents are sequences of paragraphs. Both written and spoken language are sequential in nature. RNNs are appropriate choice for sequence processing. In addition, RNNs are also quite versatile, as we have seen before. Their different architectures can be used for different NLP tasks.\n",
    "\n",
    "__Exercise 8.3:__ Can you name some NLP tasks for each basic RNN architecture (many to one, one to many, etc.) described above?\n",
    "\n",
    "But, how should we feed text into neural models? The most common way is to feed the text word by word. Each word is represented by its `id` - a unique integer identifier. We get this `id` simply by constructing a vocabulary of all the words we have in our dataset.\n",
    "\n",
    "We have some data ready for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 200\n",
      "('Kde', 'tÃ¡', 'Ä¾udskÃ¡', 'duÅ¡a', 'drieme', '?')\n",
      "('PreÄo', 'to', 'tu', 'pÃ­Å¡em', '?')\n",
      "('PreÄo', 'je', 'dneÅ¡nÃ½', 'humor', 'suchÃ½', '?')\n",
      "('Aby', 'sa', 'Ä¾udia', 'mohli', 'smiaÅ¥', '.')\n",
      "('PreÄo', 'niesme', 'prirodzenÃ­', '?')\n"
     ]
    }
   ],
   "source": [
    "dataset = data.load_pos_data('data/train')\n",
    "\n",
    "print(f'Number of samples: {len(dataset)}')\n",
    "\n",
    "for sample in dataset[:5]:\n",
    "    print(sample.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also constructed the vocabulary for you. It is simply a dictionary with words as keys and theirs `ids` as values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'Vtedy': 1, 'kategÃ³ria': 2, 'Lee': 3, 'terÃ©nnych': 4, 'chuti': 5, 'pobyt': 6, 'OdkiaÄ¾': 7, 'lÃ­stkami': 8, 'sÃºÅ¥aÅ¾': 9, 'Strong': 10, 'ChvÃ­Ä¾u': 11, 'vtedajÅ¡Ã­': 12, 'skalickom': 13, 'VÃ­Å¥az': 14, 'Uvedomovali': 15, 'retrospektÃ­vnu': 16, 'televÃ­znom': 17, 'inteligencie': 18, 'ohÄ¾ady': 19, 'dÄ›lal': 20, 'dÄºÅ¾ku': 21, 'zaÄali': 22, 'AsociÃ¡cia': 23, 'Ãºsmevom': 24, 'poÅ¾iadavky': 25, 'plodom': 26, 'bÃ¡nk': 27, 'funkciu': 28, 'Ethan': 29, 'pri': 30, 'nim': 31, 'Viera': 32, 'skalickÃ©ho': 33, 'ceremoniÃ¡lu': 34, 'vyhrabala': 35, 'Dovtedy': 36, 'bola': 37, 'dostaÅ¥': 38, 'premiÃ©ra': 39, 'stÃ¡Å¥': 40, 'maÄka': 41, 'profesor': 42, 'nerovnostÃ­': 43, 'nÃ¡zvom': 44, 'odpÃ­sala': 45, 'vyjadrovali': 46, 'nezaujÃ­mavÃ©ho': 47, 'svojho': 48, 'obchodom': 49, 'pohÄ¾adu': 50, 'ÃºmornÃ©ho': 51, 'Tom': 52, 'fotografiu': 53, 'zaoberal': 54, 'rÃ³mske': 55, 'Ostanem': 56, 'Samonafukovacie': 57, 'Ja': 58, 'prikrÃ½vky': 59, 'Internet': 60, 'myÅ¡': 61, '10': 62, 'ObyÄajne': 63, 'urÄovala': 64, 'zmluvy': 65, 'ostatnÃ©': 66, 'konfliktom': 67, 'predsa': 68, 'stalo': 69, 'duÅ¡a': 70, 'tuÅ¡il': 71, 'kaÅ¾dej': 72, 'potravÃ­n': 73, 'Adam': 74, 'bolesti': 75, 'kÃ½m': 76, 'Nemyslite': 77, 'vÃ½vinu': 78, 'existuje': 79, 'nemzet': 80, 'bezvÃ½hradne': 81, 'dovolenkou': 82, 'pÃ­saÅ¥': 83, 'dodÃ¡va': 84, 'odohrÃ¡vajÃºci': 85, 'Omyl': 86, 'nezmenenom': 87, 'zÃ¡ujemcov': 88, 'Å¾ivot': 89, 's': 90, 'vyÄkÃ¡vania': 91, 'normou': 92, 'dobrÃ½ch': 93, 'Å¾e': 94, 'gymnÃ¡ziu': 95, 'prajem': 96, 'vrstiev': 97, 'soÄ¾': 98, 'Shakespeare': 99, 'KaÅ¾dÃ½': 100, 'PiÅ¡Å¥anek': 101, 'mu': 102, 'PoÄ¾sko': 103, 'mÃ©dium': 104, 'spÃ´sobiÅ¥': 105, 'sprÃ¡va': 106, 'veÄ¾kÃº': 107, 'gramatike': 108, 'diela': 109, 'mimoriadne': 110, 'komunistickÃ¡': 111, 'Bogart': 112, 'Princ': 113, 'vychÃ¡dza': 114, 'prachovky': 115, 'nezmysel': 116, 'tenulinkÃ©': 117, 'eÅ¡te': 118, 'ostrihomskÃ½': 119, 'Kamera': 120, 'FolklÃ³rneho': 121, 'nezvyÄajnÃ©': 122, 'Eugen': 123, 'Hysteria': 124, 'kaÅ¾dÃ©mu': 125, 'obaja': 126, 'KOZ': 127, 'Äaleko': 128, 'uhorku': 129, 'boÅ¾skosti': 130, 'upreli': 131, 'spÃ¡lni': 132, 'Dnes': 133, 'Pub': 134, 'rohoÅ¾ke': 135, 'tri': 136, 'Benigni': 137, 'Jaroslav': 138, 'myÅ¡lienky': 139, 'neodmysliteÄ¾ne': 140, 'PrepÃ¡Äte': 141, 'prÃ¡vo': 142, '1999': 143, 'rÃ´znych': 144, 'prehliadky': 145, 'I': 146, 'Tomei': 147, 'poÄas': 148, 'zamyslieÅ¥': 149, 'Rado': 150, 'problÃ©my': 151, 'jednom': 152, 'priÅ¡lo': 153, 'Slovakia': 154, 'klasickÃ©': 155, 'PoslÃºchli': 156, 'To': 157, 'beletriu': 158, 'utvrdil': 159, 'vedeckej': 160, 'rozpade': 161, 'ZÃ¡ujemcov': 162, 'interrupcie': 163, 'odbornou': 164, 'nedorozumeniam': 165, 'hnusnÃ©': 166, 'America': 167, 'ramenÃ¡': 168, 'zo': 169, 'ZabezpeÄia': 170, 'vlastnÃ½': 171, 'Å¾ivotnÃ©': 172, 'vychladnÃºÅ¥': 173, 'koÅ¾u': 174, 'necelÃº': 175, 'voliÅ¥': 176, 'rozmery': 177, 'oheÅˆ': 178, 'starÃ½': 179, 'postarajÃº': 180, 'nezmenÃ­': 181, 'Naviac': 182, 'NosÃ¡Ä¾a': 183, 'turnaj': 184, 'dobrÃ½': 185, 'TenkÃ¡': 186, 'chvÃ­le': 187, 'VlastnÃ½': 188, 'KÃ½m': 189, 'jednoduchÃ¡': 190, 'filmu': 191, 'vzrÃ¡stli': 192, 'ho': 193, 'Dey': 194, 'Johnovi': 195, 'arabskej': 196, 'rovno': 197, 'prirodzene': 198, 'nÃ¡chylnÃ©': 199, 'veniec': 200, 'pouÅ¾itie': 201, 'vzÅ¥ah': 202, 'zahrÃ¡': 203, 'Scott': 204, 'spÃ¤Å¥': 205, 'organizÃ¡tor': 206, 'musieÅ¥': 207, 'umrieÅ¥': 208, 'emeritus': 209, 'balkÃ³novÃ©': 210, 'VhodnÃ¡': 211, 'nÃ¡zov': 212, 'systematicky': 213, 'otvorenÃ­': 214, 'PrÃ­tomnosÅ¥': 215, 'fotkami': 216, 'nestaÄia': 217, 'Å tefan': 218, 'mÃ¡': 219, 'BraÅˆa': 220, 'Na': 221, 'slovenskej': 222, 'narazÃ­te': 223, 'porota': 224, 'tÃ½Å¾dÅˆa': 225, 'by': 226, 'kniÅ¾nici': 227, 'tempe': 228, 'Screen': 229, 'nÃ´Å¾': 230, 'Bodka': 231, 'Ä¾ahÅ¡ie': 232, 'pricestuje': 233, 'civilizÃ¡cii': 234, 'odpoveÄ': 235, 'Toto': 236, 'recept': 237, 'bolo': 238, 'byt': 239, 'nosenia': 240, 'vÃ¤ÄÅ¡inou': 241, 'zÃ¡bavu': 242, 'KostÃ½my': 243, 'SilnÃ½': 244, 'roÄnÃ­ka': 245, 'spoloÄne': 246, 'spracovanie': 247, 'neuberie': 248, 'roÄnÃ­kov': 249, 'zÃ¡ves': 250, 'nespal': 251, 'DoÅ¡lo': 252, 'zmiznutie': 253, 'majÃº': 254, 'musÃ­m': 255, 'druhy': 256, 'Å¡tÃ¡tu': 257, 'snob': 258, 'evanjelickej': 259, 'nijako': 260, 'blbci': 261, 'beÅ¾nÃ©ho': 262, 'tÃ½chto': 263, 'tridsiatych': 264, 'sÃºÅ¥aÅ¾e': 265, 'dieÅ¥aÅ¥u': 266, 'Love': 267, 'Ä½UBOMÃRA': 268, 'zÃ¡klad': 269, 'regenerÃ¡ciu': 270, 'Meno': 271, 'skÃºsenejÅ¡Ã­': 272, 'kaÅ¾dÃº': 273, 'menu': 274, 'starÅ¡Ã­': 275, 'pouÅ¾Ã­vateÄ¾a': 276, 'inom': 277, 'chlapÃ­k': 278, 'pamÃ¤tÃ¡m': 279, 'organizÃ¡ciÃ­': 280, 'individuÃ¡lnu': 281, 'Gordon': 282, 'otvorenÃ©': 283, 'jedÃ¡lny': 284, 'stÃ¡le': 285, 'ublÃ­Å¾iÅ¥': 286, 'Grable': 287, 'ODCHODY': 288, 'snaÅ¾il': 289, 'Belushi': 290, 'nafukuje': 291, 'letÃ¡ky': 292, 'prÃ­jemnÃ©': 293, 'zÃ¡kladnÃ©ho': 294, 'pevnÃ¡': 295, 'literatÃºrou': 296, 'princÃ­pu': 297, 'mne': 298, 'tento': 299, 'zÃ­skal': 300, 'domu': 301, 'ÄŽuroviÄa': 302, 'mozno': 303, 'hry': 304, 'jednotlivÃ½ch': 305, 'vybavÃ­m': 306, 'nepÃ´jdu': 307, 'prebieha': 308, 'dÃ´stojnÃ½': 309, 'stieranie': 310, 'bohÃ©m': 311, 'HalogÃ©novÃ©': 312, 'znÃ¡my': 313, 'Harold': 314, 'stiahnite': 315, 'momentÃ¡lne': 316, 'takto': 317, 'nemuseli': 318, 'Vianoce': 319, 'naÅ¡li': 320, 'ale': 321, 'publikum': 322, 'naÅ¡i': 323, 'tentoraz': 324, 'DÃ´raz': 325, 'prÃ­jemnÃ¡': 326, 'monogamnÃ½': 327, 'Festivalu': 328, 'ÄŒlÃ¡nky': 329, 'pena': 330, 'jedineÄnÃ½': 331, 'Je': 332, 'dejinami': 333, 'automat': 334, 'Skuste': 335, 'zÃ¡pad': 336, 'Koncepciu': 337, 'vypnutÃ½m': 338, 'Joe': 339, 'Otvorila': 340, 'zaobchÃ¡dzaniu': 341, 'presvedÄenie': 342, 'folklÃ³rnych': 343, 'vyskÃºÅ¡ajte': 344, 'J': 345, 'TakÃ¡': 346, 'Sklo': 347, 'presvedÄÃ­': 348, 'vynikajÃºcom': 349, 'visel': 350, 'Å¡iju': 351, 'zadok': 352, 'vedela': 353, 'vyvÃ­jali': 354, 'Ä½ubomÃ­r': 355, 'MeÄiarov': 356, 'pocit': 357, 'mÅˆa': 358, 'mÃ´jho': 359, 'ktorÃ½': 360, 'izbovÃº': 361, 'Sorvino': 362, 'sÃºÄasnÃ½m': 363, 'obeÅ¥': 364, 'televÃ­zneho': 365, 'MoÅ¾no': 366, 'Ãno': 367, 'Ide': 368, 'obraÅ¥': 369, 'ovce': 370, 'verÃ­me': 371, 'spokojnÃ½': 372, 'nepomÃ´Å¾e': 373, 'tvaroslovie': 374, 'choreografa': 375, 'ScenÃ¡r': 376, 'Kirk': 377, 'reÅ¾isÃ©rom': 378, 'prÃ­jemnÃ½m': 379, 'konanie': 380, 'robÃ­te': 381, 'plech': 382, 'neviem': 383, 'rodiny': 384, 'ÃšspeÅ¡nÃ½': 385, 'funk': 386, 'objaviÅ¥': 387, 'neÄuduje': 388, 'Å½e': 389, 'Å¡tartujeme': 390, 'A': 391, 'tom': 392, 'PremiÃ©ra': 393, 'niet': 394, 'zloÄineckÃ¡': 395, 'maÅ¥': 396, 'krÄmami': 397, 'Jamie': 398, 'vzornÃ©ho': 399, 'DobrÃº': 400, 'nejakÃ¡': 401, 'pÃ­lku': 402, 'popruhy': 403, 'NepriÅ¡la': 404, 'pomery': 405, 'ceny': 406, 'za': 407, 'tÃ¡': 408, 'obnovenÃ©': 409, 'adrese': 410, 'toho': 411, 'JeÅ¾iÅ¡kovi': 412, 'dovolenke': 413, 'osÃ´b': 414, 'slovenÄiny': 415, 'paralela': 416, 'vzduchotesnom': 417, 'veÄ¾mi': 418, 'povstanie': 419, 'text': 420, 'vlasti': 421, 'sieni': 422, 'Sir': 423, 'hlupÃ¡ci': 424, 'hlavnej': 425, 'Obidve': 426, 'Pre': 427, 'Shore': 428, 'dospelÃ½ch': 429, 'hrÃ¡Å¡ku': 430, 'ImidÅ¾': 431, 'Åˆu': 432, 'vypnite': 433, 'prÃ­liÅ¡': 434, 'ReÅ¡pektovali': 435, 'Ãºrovni': 436, 'â€ž': 437, 'TakÅ¾e': 438, 'do': 439, 'Ich': 440, 'kola': 441, 'Ivan': 442, 'muzikÃ¡lu': 443, 'dÃ´sledkami': 444, 'niÄ': 445, 'podobu': 446, 'Miami': 447, 'SpaÄ¾ujÃºca': 448, 'Chcela': 449, 'Ä¾udia': 450, 'Otec': 451, 'odolnosÅ¥': 452, 'hlasy': 453, 'favorita': 454, 'aby': 455, 'siahnuÅ¥': 456, 'neskÃ´r': 457, 'farÃ¡rov': 458, 'ProfilovanÃ©': 459, 'akÃ©': 460, 'kde': 461, 'poÄÃ­taÄu': 462, 'Blaho': 463, 'HlbokÃ½': 464, 'SamozrejmosÅ¥ou': 465, 'odriekla': 466, 'alebo': 467, 'vyrieÅ¡enÃ¡': 468, 'ku': 469, 'Betty': 470, 'VaÅ¡e': 471, 'nepovedal': 472, 'neznÃ¡Å¡ate': 473, 'stojÃ­': 474, 'teÃ³rie': 475, 'takÃ½to': 476, 'podobe': 477, 'hrÃ¡': 478, 'klube': 479, 'princeznÃ¡': 480, 'berÃº': 481, 'chytÃ­': 482, 'presne': 483, 'kinematografiÃ­': 484, 'dopadne': 485, 'veÄer': 486, 'zoskupenia': 487, 'mohli': 488, 'Chcel': 489, 'NÃ¡hradnÃ©': 490, 'ste': 491, 'treba': 492, 'konzula': 493, 'krutÃ¡': 494, 'zbaviÅ¥': 495, 'rozviedla': 496, 'TeÅ¡Ã­m': 497, 'krajÃ­n': 498, 'POZOR': 499, 'dovolil': 500, 'prirodzenÃ½m': 501, 'kolegyÅˆa': 502, 'Å¾iarovky': 503, 'â€œ': 504, 'konkrÃ©tne': 505, 'upratovaÄku': 506, 'nikto': 507, 'Marian': 508, 'minulosti': 509, 'vÅ¾dy': 510, 'tÃ­to': 511, 'nÃ¡klady': 512, 'sama': 513, 'NiÄ': 514, 'VÃ½stava': 515, 'dom': 516, 'plastu': 517, 'napÃ­sal': 518, 'celÃ©ho': 519, 'duÅ¡e': 520, 'Å¥aÅ¾kÃ¡': 521, 'â€': 522, 'Light': 523, 'rÃ³mskej': 524, 'KniÅ¾nom': 525, '13': 526, 'Napokon': 527, 'nerÃ³mske': 528, 'poÅ¾iadavkami': 529, 'sÃ©riu': 530, 'obsahovaÅ¥': 531, 'rokoch': 532, 'zÃ¡Å¥aÅ¾': 533, 'veÄ¾a': 534, 'Igor': 535, 'z': 536, 'vysÃ¡vaÄom': 537, 'bezprizornÃ¡': 538, 'problÃ©m': 539, 'uveriÅ¥': 540, 'DoleÅ¾alova': 541, 'Goldsmith': 542, 'jedinÃ¡': 543, 'tomto': 544, 'zmena': 545, 'Platba': 546, 'Å¾ivotnÃº': 547, 'sÃºkromie': 548, 'Ä¾udovej': 549, 'svojej': 550, 'vÅ¡etky': 551, 'dÃ´kaz': 552, 'Medzi': 553, 'zrak': 554, 'Plum': 555, 'MikulÃ¡Å¡i': 556, 'pokrievkou': 557, 'kritickÃº': 558, 'nÃ¡stupca': 559, 'vÃ¤ÄÅ¡inu': 560, 'VeÄer': 561, 'len': 562, 'diera': 563, 'futbalovÃ½': 564, 'nudnÃ©ho': 565, 'poslednÃ©': 566, 'JednÃ©ho': 567, 'udeÄ¾ovanÃ©': 568, 'so': 569, 'inÃ©': 570, 'S': 571, 'uvedomil': 572, 'ktorÃ¡': 573, 'rastie': 574, 'zaujÃ­mavÃ½': 575, 'rieÅ¡enie': 576, 'poteÅ¡ia': 577, 'Ãºvod': 578, 'nÃ¡dejou': 579, 'prvej': 580, 'bratislavskÃ½mi': 581, 'vÃ½razne': 582, 'zmiznÃºÅ¥': 583, 'poÄtom': 584, 'Bacall': 585, 'pudenie': 586, 'pÃ­Å¡em': 587, 'slepÃº': 588, 'jednoduchÃ½': 589, 'Eva': 590, 'tu': 591, 'VÃ½chodnej': 592, 'Ruth': 593, 'presvedÄia': 594, 'prÃ­sÅ¥': 595, 'LÃºÄnice': 596, 'vyhrÃ¡vaÅ¥': 597, 'Piateho': 598, 'Zase': 599, 'olej': 600, 'fundamentalizmus': 601, 'Primas': 602, 'izolÃ¡cia': 603, 'SlovenskÃ½': 604, 'wind': 605, 'dcÃ©ru': 606, 'humor': 607, 'kancelÃ¡rie': 608, 'Humphrey': 609, 'varovala': 610, 'fotografiÃ­': 611, 'Opusu': 612, 'firme': 613, 'HrÃ¡': 614, 'dÃ¡vaÅ¥': 615, 'koncom': 616, 'KonkrÃ©tne': 617, 'univerzÃ¡lnosÅ¥ou': 618, 'naozaj': 619, 'Tentoraz': 620, 'odrÃ¡Å¾ali': 621, 'zÃ¡bava': 622, 'Å tÃºrovskÃ½': 623, 'naprÃ­klad': 624, 'nebol': 625, 'relÃ¡cia': 626, 'festival': 627, 'komÃ©dii': 628, 'zmanipulovaÅ¥': 629, 'pohode': 630, 'OcenÃ­': 631, 'nÃ¡m': 632, 'nevznikla': 633, 'mÃ´Å¾e': 634, 'zaujÃ­mavosti': 635, 'kniÅ¾nej': 636, 'vÃ½znamoch': 637, 'zbierky': 638, 'kamarÃ¡tov': 639, 'zaujÃ­mavÃ¡': 640, 'uvÃ¡dzaÅ¥': 641, 'Pri': 642, 'epizÃ³d': 643, 'kategÃ³rii': 644, 'kontinuum': 645, 'zmes': 646, 'Odhrnula': 647, 'stokrÃ¡t': 648, 'ÄŒeskÃº': 649, 'Trek': 650, 'festivale': 651, 'nebude': 652, 'teda': 653, 'istej': 654, 'pogumovanÃ½m': 655, 'netradiÄnÃº': 656, 'vojaka': 657, 'vÃ½skume': 658, 'najistejÅ¡ie': 659, 'symbolom': 660, 'teÅ¡Ã­m': 661, 'zÃ³ny': 662, 'bulla': 663, 'CelÃ½': 664, 'urÄenÃ¡': 665, 'generÃ¡ciu': 666, '-': 667, 'MladÃ¡': 668, 'vÃ½znamnej': 669, 'kroku': 670, 'zmieÅ¡ajte': 671, 'pripomienku': 672, 'jazyk': 673, 'zÃ­skava': 674, 'bude': 675, 'Bude': 676, 'Ãºtok': 677, 'pÃ­sala': 678, 'bratia': 679, 'filmovÃ½': 680, 'zÃ¡palky': 681, 'DieÅ¥a': 682, 'kontroverznÃ©': 683, 'neplytvaÅ¥': 684, 'Bullock': 685, 'nÃ¡s': 686, 'Tak': 687, 'Pred': 688, 'podarilo': 689, 'baterku': 690, 'zvÃ­Å¥azila': 691, 'webovÃº': 692, 'jasnÃ©': 693, 'Jaskyne': 694, 'vÃ½znamnÃ©': 695, 'hudobnÃ©': 696, 'koÄ¾ko': 697, 'menÃ­': 698, 'niekoÄ¾ko': 699, 'chcela': 700, 'nieÄo': 701, 'prÃ­jemnÃ½': 702, 'stranu': 703, 'Asi': 704, 'Nie': 705, 'RidzoÅˆ': 706, 'celÃ½': 707, 'reprezentovaÅ¥': 708, 'mÃºzea': 709, 'dvere': 710, 'preÄ': 711, 'svetlo': 712, 'seriÃ¡l': 713, 'sportu': 714, 'Inak': 715, 'rock': 716, 'ZachrÃ¡Åˆte': 717, 'strany': 718, 'BERLINALE': 719, 'Gay': 720, 'Å¡alÃ¡tom': 721, 'potrebu': 722, 'fotoaparÃ¡tov': 723, 'Italia': 724, 'konzervy': 725, 'pochÃ¡dzala': 726, 'pokiaÄ¾': 727, 'Åˆom': 728, 'Å¡kÃ´l': 729, 'samozrejmosÅ¥ou': 730, 'iÅ¡iel': 731, 'vÃ½beru': 732, 'stÃ´l': 733, 'Foto': 734, 'Marisa': 735, 'prvÃ©ho': 736, 'kuskus': 737, 'Saktor': 738, 'ilustraÄnÃ©': 739, 'Jazz': 740, 'Sklamanie': 741, 'ventil': 742, 'Hudba': 743, 'aÅ¾': 744, 'Preto': 745, 'unesenÃº': 746, 'Svet': 747, 'minule': 748, 'NaÅ¡Å¥astie': 749, 'recenzie': 750, 'NÃ¡Å¡': 751, 'storoÄÃ­': 752, 'penovÃ©ho': 753, 'dierovaÄ': 754, 'paradigmu': 755, 'Nezabudnite': 756, 'spisovnej': 757, 'SnÃ¡Ä': 758, 'hudobnÃ½ch': 759, 'Ä¾ÃºbostnÃ½': 760, 'ÄerstvÃ½': 761, 'TÃ¡to': 762, 'organizÃ¡cii': 763, 'roky': 764, 'ÃºspeÅ¡nÃ©ho': 765, 'znaÄkÃ¡ch': 766, 'mise': 767, 'VieÅ¡': 768, 'zomrel': 769, 'VÄera': 770, 'Donen': 771, 'Stephen': 772, 'paradoxnÃ½m': 773, 'dÃ¡': 774, 'republiku': 775, 'pamÃ¤ti': 776, 'MeÄiar': 777, 'Lauren': 778, 'NÃ´Å¾': 779, 'gramatickom': 780, 'priatelia': 781, 'Laurence': 782, 'k': 783, 'taÅ¡ky': 784, 'zÃ¡stupcov': 785, 'nepriatelia': 786, 'VÃ½uÄbu': 787, 'otcove': 788, 'KrÃ¡la': 789, 'spÃ´sob': 790, 'Jano': 791, 'vracia': 792, 'skupÃ­n': 793, 'archÃ©': 794, 'jeden': 795, 'nÃ¡deje': 796, '9': 797, 'cieÄ¾om': 798, 'zas': 799, 'mesiac': 800, 'tÃºÅ¾ba': 801, 'OndÅ™ejÃ­Äek': 802, 'minulÃ½': 803, 'MuziÄka': 804, 'zvyknÃº': 805, 'cibuÄ¾u': 806, 'novinÃ¡r': 807, 'mi': 808, 'cirkusu': 809, 'Medlen': 810, 'Pickford': 811, 'VÃ©Äka': 812, 'prezident': 813, 'vojny': 814, 'preÅ¾Ã­va': 815, 'jednu': 816, 'tak': 817, 'tÃ½ch': 818, 'Å½elÃ¡m': 819, 'PÃ´vodnÃ½': 820, 'spomÃ­nal': 821, 'KrÃ¡sne': 822, 'nevydrÅ¾al': 823, 'skrachovala': 824, 'Taniere': 825, 'odmietnutÃ½': 826, 'BerlÃ­ne': 827, 'prehliadke': 828, 'pritom': 829, 'si': 830, 'petrÅ¾lenovÃº': 831, 'otvorenÃ½mi': 832, 'zloÄineckÃ½mi': 833, 'ZÃ¡hradkÃ¡ch': 834, 'ani': 835, 'zaberie': 836, 'NaÅ¡e': 837, 'chcieÅ¥': 838, 'povedie': 839, 'prÃ­stavbu': 840, 'DivokÃ½': 841, 'vÃ½sledok': 842, 'snaÅ¾Ã­': 843, 'Å½iÅ¥': 844, 'egyptskÃ½': 845, 'Mimoriadne': 846, 'nejakej': 847, 'NevidieÅ¥': 848, 'nej': 849, 'Rovnako': 850, 'veÄery': 851, 'aj': 852, 'Hodili': 853, 'Sandry': 854, 'vaÅ¡e': 855, '2': 856, 'VÃ½prava': 857, 'totÃ¡lne': 858, 'ZÃ¡vereÄnÃ¡': 859, 'Å¡peciÃ¡lneho': 860, 'toto': 861, 'Sv': 862, 'musÃ­': 863, 'UÅ¾': 864, 'pÃ³rmi': 865, 'umeleckÃ©': 866, 'Tie': 867, 'UhorskÃ©ho': 868, 'neexistovalo': 869, 'NemÃ´Å¾em': 870, 'siedmich': 871, 'oveÄ¾a': 872, 'Chrisom': 873, 'unifikÃ¡ciu': 874, 'stolu': 875, 'oÄakÃ¡vajÃº': 876, 'King': 877, 'zeleninovÃº': 878, 'neodporÃºÄam': 879, 'servÃ­rujte': 880, 'ÄeskÃ½': 881, 'donekoneÄna': 882, 'Ä¾ubovoÄ¾ne': 883, 'TaneÄnÃ½': 884, 'pre': 885, 'slovo': 886, 'Dante': 887, 'ponuky': 888, 'systÃ©me': 889, 'mnohokrÃ¡t': 890, 'Äias': 891, 'DÃ´vod': 892, 'rozprÃ¡vky': 893, 'pohÄ¾adom': 894, 'Å ikovne': 895, 'David': 896, 'spisovnÃ½': 897, 'AmeriÄania': 898, 'MÃ¡me': 899, 'dosÅ¥': 900, 'dostatoÄne': 901, 'najbliÅ¾Å¡ieho': 902, 'ktorÃ©ho': 903, 'Gavin': 904, 'funguje': 905, 'hrdinskÃ½m': 906, 'Äakal': 907, 'rovnicou': 908, 'Gillian': 909, 'vyzdvihnutÃ­': 910, 'Aby': 911, 'skonÄilo': 912, 'DobrovoÄ¾nÃ­ci': 913, 'unikÃ¡tna': 914, 'Chan': 915, 'fotiek': 916, 'Å¾iarovka': 917, 'bohatÃ½': 918, 'rozmermi': 919, 'preÄo': 920, 'otca': 921, 'hudbe': 922, 'dve': 923, 'tvrdiÅ¥': 924, 'vrÃ¡tili': 925, 'strÃ¡nke': 926, 'prihodiÅ¥': 927, 'Program': 928, 'vÅ¡etkÃ½ch': 929, 'Stanley': 930, 'U': 931, 'inÃ½': 932, 'potrvÃ¡': 933, 'kandidÃ¡tmi': 934, 'Ä¾ahkÃ©': 935, 'zÃ¡leÅ¾itosÅ¥': 936, 'to': 937, 'STOROÄŒIA': 938, 'SkÃºste': 939, 'vtedy': 940, 'Uher': 941, 'kategÃ³rie': 942, 'knihu': 943, 'Jan': 944, 'Elena': 945, 'pinzetu': 946, 'vyuÅ¾il': 947, 'hereckÃ½mi': 948, 'sused': 949, 'Noon': 950, 'batÃ©rie': 951, 'NiektorÃ©': 952, 'vÃ½raznejÅ¡Ã­': 953, 'pupok': 954, 'ZlatÃ½': 955, 'ÄŒeskoslovensko': 956, 'nepoznali': 957, 'kriÄal': 958, 'totiÅ¾': 959, 'vlhkosti': 960, 'zaÄalo': 961, '1': 962, 'priÅ¡iel': 963, 'vyzerÃ¡': 964, 'hlÃ¡skoslovie': 965, 'vysvetliÅ¥': 966, 'rovesnÃ­ci': 967, 'Ä½ahkÃ¡': 968, 'problÃ©mom': 969, 'blÃ­zko': 970, 'AlÅ¾beta': 971, 'Ray': 972, 'mal': 973, 'ÄŒiapky': 974, 'slovenskom': 975, 'Film': 976, 'Slovenska': 977, 'energie': 978, 'jazda': 979, 'Ã­sÅ¥': 980, 'Å¡tyroch': 981, 'vÃ¡Å¾ne': 982, 'Poitier': 983, 'Å¡irokÃ©': 984, 'patria': 985, 'budem': 986, 'pohyb': 987, 'ZÃ¡mer': 988, 'mnoÅ¾stvo': 989, 'oÄi': 990, 'Telo': 991, 'Courtney': 992, 'skÃ¡l': 993, 'svojom': 994, 'vÃ¡m': 995, 'tradÃ­ciou': 996, 'aprÃ­li': 997, 'dieÅ¥a': 998, 'ÄÃ­nskeho': 999, 'tvrdila': 1000, 'znÃ¡mymi': 1001, 'deti': 1002, 'ikona': 1003, 'Pozdravuj': 1004, 'ich': 1005, 'vlastne': 1006, 'prirodzenÃ­': 1007, 'stupÅˆom': 1008, 'celÃ©': 1009, 'Å¡edÃ½': 1010, 'Kto': 1011, 'podmienkou': 1012, 'HvoreckÃ½': 1013, 'zmestÃ­': 1014, 'drevo': 1015, 'Obsiahly': 1016, 'rokov': 1017, 'adventnÃ½': 1018, 'povrchom': 1019, 'zaÄaÅ¥': 1020, 'Korbel': 1021, 'TradiÄnÃº': 1022, 'vietor': 1023, 'druhov': 1024, 'otvorilo': 1025, 'mali': 1026, 'ak': 1027, 'rÃ´znu': 1028, 'sexuÃ¡lne': 1029, 'nasadenie': 1030, 'byÅ¥': 1031, 'Bonus': 1032, 'malo': 1033, 'drsnÃ©mu': 1034, 'kolega': 1035, 'popisovanÃ­': 1036, 'tomu': 1037, 'kaviarni': 1038, 'PoÅ¡li': 1039, 'snov': 1040, 'ZaslanÃ©': 1041, 'Lucille': 1042, 'mÃ¡me': 1043, 'kaktus': 1044, 'najlepÅ¡iu': 1045, 'ruke': 1046, 'zorganizovaÅ¥': 1047, 'vrchole': 1048, 'lÃ¡kali': 1049, 'prvÃ©': 1050, 'Ãºstav': 1051, 'debutujÃºci': 1052, 'sekciu': 1053, 'bombardovaÅ¥': 1054, 'vÃ´bec': 1055, 'NakrÃºcanie': 1056, 'ZaÄal': 1057, 'hlavy': 1058, 'dopadol': 1059, 'chodili': 1060, 'Äasu': 1061, '1992': 1062, 'pÃ³dia': 1063, 'histÃ³rie': 1064, 'najznÃ¡mejÅ¡Ã­ch': 1065, 'nahrÃ¡vaÅ¥': 1066, 'adaptÃ¡cia': 1067, 'kultÃºrne': 1068, 'ktorej': 1069, 'cudziu': 1070, 'Niekde': 1071, 'Bel': 1072, 'jari': 1073, 'bazalku': 1074, 'veÄ¾kej': 1075, 'druhom': 1076, 'nezaruÄuje': 1077, ')': 1078, 'klubov': 1079, 'Vlani': 1080, 'poÅ¾iadate': 1081, 'prvÃ½m': 1082, 'mena': 1083, 'jesene': 1084, 'Alebo': 1085, 'ozÃ½vala': 1086, 'Zakryte': 1087, 'letu': 1088, 'Ãºplne': 1089, 'dokladaÅ¥': 1090, 'kupoval': 1091, 'bernolÃ¡kovcov': 1092, 'divadle': 1093, 'Bratislave': 1094, 'Oldmanom': 1095, 'jedla': 1096, 'znova': 1097, 'pohybe': 1098, 'tancov': 1099, 'vajÃ­Äko': 1100, 'dopracovÃ¡vajÃº': 1101, 'Hanbila': 1102, 'rohu': 1103, 'Ãºlohy': 1104, 'slovenskÃº': 1105, 'SlovenskÃ½ch': 1106, 'PDF': 1107, 'rubrike': 1108, 'podloÅ¾iek': 1109, 'Z': 1110, 'dennÃ­ky': 1111, 'ÄŒÃ­Åˆana': 1112, 'obsadiÅ¥': 1113, 'podieÄ¾a': 1114, 'rozhlasu': 1115, 'cirkevnej': 1116, 'trivia': 1117, 'hranÃ­c': 1118, 'herec': 1119, 'vysokÃ¡': 1120, 'Noviny': 1121, '\"': 1122, 'lete': 1123, 'vhodnÃ©': 1124, 'Do': 1125, 'karimatky': 1126, 'modernÃº': 1127, 'bliÅ¾Å¡ie': 1128, 'divadelnÃ©': 1129, 'javiska': 1130, 'nechÃ¡peme': 1131, 'MK': 1132, 'odohrÃ¡va': 1133, 'uzatvorenÃ½': 1134, 'po': 1135, 'fiktÃ­vnom': 1136, 'vrÃ¡time': 1137, 'ÄeÅ¡tiny': 1138, 'efekty': 1139, 'Shanghai': 1140, 'Äeskej': 1141, 'podloÅ¾ky': 1142, 'MenÅ¡ie': 1143, 'pripravili': 1144, 'Garbo': 1145, 'gramatika': 1146, 'jedinÃ©': 1147, 'nenÃ¡roÄnÃ©': 1148, 'drÅ¾ala': 1149, 'naklonenÃ­': 1150, 'tepelnÃ¡': 1151, 'dorozumievania': 1152, 'tady': 1153, 'dnes': 1154, 'blÃ­zkosti': 1155, 'pred': 1156, 'fÄ¾aÅ¡e': 1157, '6': 1158, 'Ferda': 1159, 'napÄºÅˆajÃº': 1160, 'kole': 1161, 'detstve': 1162, 'nereÅ¾Ãº': 1163, 'potuluje': 1164, 'gÃ³l': 1165, 'JaslovskÃ½': 1166, 'prostrednÃ­ctvom': 1167, 'narodenie': 1168, 'rozbiÅ¥': 1169, 'stopÃ¡rov': 1170, 'obchod': 1171, 'priateÄ¾': 1172, 'kaÅ¾dÃ½': 1173, 'tradiÄne': 1174, 'DruhÃ¡': 1175, 'TÃ¡': 1176, 'vytvÃ¡raÅ¥': 1177, 'elektronickÃ½ch': 1178, 'sebou': 1179, 'reÄ': 1180, 'dni': 1181, 'podpÃ­sali': 1182, 'LÃ­stok': 1183, 'zÃ¡sadne': 1184, 'ukÃ¡zalo': 1185, 'drahÃ©': 1186, 'projekt': 1187, 'kralickÃ¡': 1188, 'vodovzdornÃ©': 1189, 'vÅ¡ak': 1190, 'tohto': 1191, 'dobre': 1192, 'tÃ½Å¾deÅˆ': 1193, 'zatiaÄ¾': 1194, 'pravom': 1195, 'kultÃºru': 1196, 'literÃ¡rnovednom': 1197, 'kultÃºrnej': 1198, 'jedna': 1199, 'stihlo': 1200, 'spÃ¡nok': 1201, 'vedia': 1202, 'Actors': 1203, 'Äloveka': 1204, 'mala': 1205, 'UmoÅ¾niÅ¥': 1206, 'televÃ­ziu': 1207, 'Anderson': 1208, \"'\": 1209, 'nasÃ¡va': 1210, 'podmienky': 1211, 'spravodlivÃ½': 1212, 'slÃ¡vnostnÃ©ho': 1213, 'opÃ¤Å¥': 1214, 'eurÃ³pskej': 1215, 'balenia': 1216, 'najlepÅ¡ie': 1217, 'novÃ©ho': 1218, 'HVIEZDY': 1219, 'RadikÃ¡lnou': 1220, 'udeÄ¾uje': 1221, 'osobitnÃ¡': 1222, 'nohy': 1223, 'ZÃ¡horskÃ©ho': 1224, 'medzi': 1225, 'musia': 1226, 'inovÃ¡ciou': 1227, 'dÅˆa': 1228, 'Nakoniec': 1229, 'Maas': 1230, 'iÅ¡li': 1231, 'systÃ©m': 1232, 'motivÃ¡cie': 1233, 'starÃ©ho': 1234, '1963': 1235, 'prÃ¡ce': 1236, 'lokality': 1237, 'svojou': 1238, 'Douglas': 1239, 'sÃºborov': 1240, 'Jenny': 1241, 'staÄÃ­': 1242, 'Obaja': 1243, 'Å¡alÃ¡tovej': 1244, 'ÄŒesko': 1245, 'vzÅ¥ahy': 1246, 'PodÄ¾a': 1247, 'tejto': 1248, 'RÃ©Å¾ia': 1249, 'dneÅ¡nÃ½': 1250, 'sÃº': 1251, 'V': 1252, 'pravdu': 1253, 'posiela': 1254, 'ReÅ¾Ã­ruje': 1255, 'Luxemburgu': 1256, 'Matej': 1257, 'NajlepÅ¡ie': 1258, 'vyhrÃ¡vam': 1259, 'debutov': 1260, 'Sharks': 1261, 'nÃ¡zvy': 1262, '7': 1263, 'videl': 1264, 'VÅ¡imli': 1265, 'vlastivednÃ½': 1266, 'nevyhÃ½bali': 1267, 'prinÃ¡Å¡a': 1268, 'hereckÃ©': 1269, 'Å irokÃ½': 1270, 'vec': 1271, 'ruÅ¾e': 1272, 'elegantne': 1273, 'nepriateÄ¾skÃ½m': 1274, 'najbezpeÄnejÅ¡ie': 1275, 'nemusÃ­m': 1276, 'Jednu': 1277, 'jazyku': 1278, 'Greta': 1279, 'lebo': 1280, 'Äeskoslovenskou': 1281, 'mieste': 1282, 'fotografickej': 1283, 'StÃ¡le': 1284, 'Jerry': 1285, 'kompromisom': 1286, 'troch': 1287, 'NemÃ¡m': 1288, 'Å¡karedÃ½m': 1289, 'zabudli': 1290, 'jednoducho': 1291, 'programovo': 1292, 'farizejstva': 1293, 'uÅ¡etrenÃ­': 1294, 'zÃ¡kladom': 1295, 'niesme': 1296, 'ÄŒo': 1297, 'v': 1298, 'hladnÃ­': 1299, 'obrazovky': 1300, 'BratislavÄanov': 1301, 'zoznam': 1302, 'logicky': 1303, 'Bavilo': 1304, 'jÃºni': 1305, 'akumulÃ¡tor': 1306, 'Nikto': 1307, 'prÃ­kladom': 1308, 'osudu': 1309, '3': 1310, 'lÃ­stok': 1311, 'ÄÃ­taÅ¥': 1312, '21': 1313, 'mimozemÅ¡Å¥ania': 1314, 'otvÃ¡raÄ': 1315, ',': 1316, 'cestuje': 1317, 'nÃ¡dobu': 1318, 'AkÃ¡csovÃ¡': 1319, 'mojom': 1320, 'zastÃ¡vke': 1321, 'zahraniÄnÃ©': 1322, 'teplÃ©ho': 1323, 'nefungujÃº': 1324, 'ohrozujÃºce': 1325, 'naraz': 1326, 'Prahe': 1327, 'Ceny': 1328, 'Sidney': 1329, 'Carterom': 1330, 'ospravedlÅˆujÃºco': 1331, 'nevstÃºpil': 1332, 'obale': 1333, 'naÅˆ': 1334, 'fungovania': 1335, 'John': 1336, 'informÃ¡ciu': 1337, 'Ä¾ahkÃ©ho': 1338, 'zachrÃ¡nil': 1339, 'otvÃ¡ral': 1340, 'pohodlÃ­m': 1341, 'ponuke': 1342, 'Kde': 1343, 'Äiara': 1344, 'Som': 1345, 'autorke': 1346, 'povysÃ¡vala': 1347, 'od': 1348, 'ZÃ¡merom': 1349, 'Svojho': 1350, 'poste': 1351, 'reÅ¾isÃ©r': 1352, 'vlastnÃ©ho': 1353, 'donedÃ¡vna': 1354, 'Dozvedel': 1355, 'zaujal': 1356, 'ZamilovanÃ½': 1357, 'ÄŒermÃ¡k': 1358, 'PriznÃ¡m': 1359, 'nÃ¡plÅˆ': 1360, 'proti': 1361, 'rezervnÃ¡': 1362, 'nami': 1363, 'Tento': 1364, 'dodnes': 1365, 'blues': 1366, 'jazykov': 1367, 'generÃ¡ciami': 1368, 'dva': 1369, 'vynikajÃºce': 1370, '!': 1371, 'nepÃ¡Äil': 1372, 'tÃ½Å¾dÅˆov': 1373, 'rade': 1374, 'obsahuje': 1375, 'debut': 1376, 'Ostala': 1377, 'zÃ¡hradu': 1378, 'dÃ¡vnejÅ¡ie': 1379, 'tÃ­': 1380, 'zvyÅ¡ok': 1381, 'zaÄiatkom': 1382, 'hluchÃº': 1383, 'PublikÃ¡ciu': 1384, 'UvidÃ­me': 1385, 'dielneho': 1386, 'KTO': 1387, 'rezervujte': 1388, 'Å¡karedÃº': 1389, 'neter': 1390, 'strana': 1391, 'zjednoduÅ¡enÃ©': 1392, 'ZÃ¡kladnÃ½mi': 1393, 'Äasopis': 1394, 'sedieÅ¥': 1395, 'sme': 1396, 'internetbankingom': 1397, 'PravdivÃ¡': 1398, 'HonorÃ¡re': 1399, 'VeÄ': 1400, 'KeÄÅ¾e': 1401, ':': 1402, 'matka': 1403, 'voÄ¾bami': 1404, 'uvÃ­ta': 1405, 'obdobie': 1406, 'mieni': 1407, 'dospeli': 1408, 'ju': 1409, 'balenÃ­': 1410, 'predstavia': 1411, 'TechnologickÃ©': 1412, 'utkveli': 1413, 'doklady': 1414, 'uÅ¾': 1415, 'CD': 1416, 'zaÅ¾iadal': 1417, 'povedala': 1418, 'Uma': 1419, 'uÄebnice': 1420, 'naÅˆho': 1421, 'VizuÃ¡lne': 1422, 'Polovicu': 1423, 'dojem': 1424, 'staÅ¥': 1425, 'vÃ½zva': 1426, 'vzduch': 1427, 'jesÅ¥': 1428, 'kontexte': 1429, 'je': 1430, 'utiahla': 1431, 'naÅ¡u': 1432, 'Boja': 1433, 'DoleÅ¾alovej': 1434, 'vÃ½hodnÃ©': 1435, 'zazrela': 1436, 'Freudom': 1437, 'negramotnÃ©': 1438, 'rovnocennÃ½': 1439, 'prilÃ¡ka': 1440, 'mystifikÃ¡ciu': 1441, 'teplotu': 1442, 'nÃ¡zornÃ½': 1443, 'volÃ¡': 1444, 'Ku': 1445, 'Profesor': 1446, 'pohÄ¾ad': 1447, 'dej': 1448, 'spomÃ­nanou': 1449, 'maÄke': 1450, 'dlho': 1451, 'spÃ´sobmi': 1452, 'predovÅ¡etkÃ½m': 1453, 'Äi': 1454, 'roku': 1455, 'Zavolajte': 1456, 'maÄku': 1457, 'NajznÃ¡mejÅ¡Ã­m': 1458, 'dieÅ¥aÅ¥a': 1459, '?': 1460, 'Guild': 1461, 'akosi': 1462, 'hrali': 1463, 'PÃ´vodnÃ¡': 1464, 'stopy': 1465, 'of': 1466, 'okamih': 1467, 'profily': 1468, 'Å¡port': 1469, 'Praefatio': 1470, 'prekvapenÃ­': 1471, 'lezie': 1472, 'slovanskÃ½ch': 1473, 'takÃ©': 1474, 'Å atana': 1475, 'minimÃ¡lne': 1476, 'Äeskou': 1477, 'knihy': 1478, 'prÃ­stupnÃ½': 1479, 'publicista': 1480, 'vyberie': 1481, 'OdolnÃ©': 1482, 'poznatkom': 1483, 'kultÃºre': 1484, 'nepravdepodobnÃ©': 1485, 'ÄasÅ¥': 1486, 'upozornil': 1487, 'Gary': 1488, 'SR': 1489, 'Keby': 1490, 'spestrenÃ­m': 1491, 'registrÃ¡ciu': 1492, 'maÄarskÃ©': 1493, 'malÃ½mi': 1494, 'rÃ¡no': 1495, 'jednej': 1496, 'synovec': 1497, 'kÃ­n': 1498, 'rÃºk': 1499, 'NebyÅ¥': 1500, 'Olivier': 1501, 'potrebujeme': 1502, 'ÄepeÄ¾': 1503, 'GlobalizÃ¡cia': 1504, 'komunistickej': 1505, 'vstÃºpi': 1506, 'spektakulÃ¡rne': 1507, 'dvadsaÅ¥': 1508, 'ukonÄenÃ­': 1509, 'fare': 1510, 'kine': 1511, 'Bol': 1512, 'novÃ½': 1513, 'Rifkin': 1514, 'rozprÃ¡val': 1515, 'kina': 1516, 'Elfman': 1517, 'â€“': 1518, 'all': 1519, 'Mary': 1520, 'vÃ¤ÄÅ¡Ã­': 1521, 'Zvuk': 1522, 'vyvolanÃ½mi': 1523, 'Reebok': 1524, 'prezidentom': 1525, 'Ypsilon': 1526, 'rydlo': 1527, 'ÄŽuroviÄ': 1528, 'nevidel': 1529, 'JÃ³na': 1530, 'skutoÄnÃ½m': 1531, 'Mama': 1532, 'znÃ¡meho': 1533, 'prirodzenÃ©': 1534, 'internetovÃº': 1535, 'Ako': 1536, 'skutoÄnÃ½': 1537, 'USA': 1538, 'ÄitateÄ¾ov': 1539, 'jazyka': 1540, 'producentka': 1541, 'BeÅ¾nÃ©': 1542, 'spolu': 1543, 'jasno': 1544, 'balkÃ³ne': 1545, 'vie': 1546, 'sprÃ¡vnu': 1547, 'duchu': 1548, 'NegatÃ­vnych': 1549, 'Å vankmajer': 1550, 'mÃ¡ja': 1551, 'vlÃ¡dnutia': 1552, 'prostredÃ­': 1553, 'reformy': 1554, 'KritÃ©rium': 1555, 'VreckovÃ½': 1556, 'Jeho': 1557, 'jej': 1558, 'roÄnÃ­k': 1559, 'novÃº': 1560, 'hlÃ¡vkovÃ½m': 1561, 'holandskÃ½': 1562, 'sociÃ¡lny': 1563, 'Å½iaden': 1564, 'Slovensko': 1565, '1968': 1566, 'Faktom': 1567, 'svoju': 1568, 'Za': 1569, 'zÃ¡kladnÃ©': 1570, 'urÄite': 1571, 'bol': 1572, 'MÃ¡': 1573, 'vzletnÃ½ch': 1574, 'dvora': 1575, 'Ãºspechu': 1576, 'reÅ¾isÃ©rskych': 1577, 'Lamar': 1578, 'vedomie': 1579, 'stÃ¡roÄnÃ©ho': 1580, 'ambiciÃ³zna': 1581, 'RuÄÃ­m': 1582, 'kaÅ¾dom': 1583, 'vÃ­Å¥az': 1584, 'Jennifer': 1585, 'dielne': 1586, 'Irak': 1587, 'zle': 1588, 'mÃ´Å¾eme': 1589, 'rÃ¡dia': 1590, 'nesÃºvisia': 1591, 'buÄte': 1592, 'doma': 1593, 'albumu': 1594, 'ruksaku': 1595, 'MÃ´j': 1596, 'telefÃ³n': 1597, 'vÅ¡etkÃ½m': 1598, 'KeÄ': 1599, 'SÃºÄasÅ¥ou': 1600, 'ankety': 1601, 'vaÅ¡u': 1602, 'vypÃ¡tranÃ½': 1603, 'drÅ¾Ã­': 1604, 'definitÃ­vne': 1605, 'Moji': 1606, 'vÃ¡s': 1607, 'povedaÅ¥': 1608, 'Stane': 1609, 'hviezd': 1610, 'sÃºbore': 1611, 'vÅ¡etko': 1612, 'neznesiteÄ¾nÃ½': 1613, 'BoruÅ¡oviÄovÃ¡': 1614, 'hudba': 1615, 'PrÃ¡ve': 1616, 'burina': 1617, 'obÄana': 1618, 'jeho': 1619, 'hovorÃ­': 1620, 'smiaÅ¥': 1621, 'som': 1622, 'Lloyd': 1623, 'keÄ': 1624, 'vlastnostÃ­': 1625, 'strÃ¡nok': 1626, 'osobne': 1627, 'ÄervenÃ¡': 1628, 'asi': 1629, 'obsahovÃ¡': 1630, 'jÃºl': 1631, 'celkom': 1632, 'radikÃ¡lnou': 1633, '1948': 1634, 'DVD': 1635, 'skoro': 1636, 'dvoch': 1637, 'Vysypte': 1638, 'novÃ©': 1639, 'nie': 1640, 'nÃ¡jdete': 1641, 'suverÃ©nny': 1642, 'hovorÃ­m': 1643, 'muÅ¾stve': 1644, 'tÃº': 1645, 'ocenenie': 1646, 'pouÅ¾Ã­vanÃ­': 1647, 'zboÅ¾Å¡Å¥ovania': 1648, 'festivalu': 1649, 'filmy': 1650, 'a': 1651, 'povedalo': 1652, 'mÃ´Å¾ete': 1653, 'SvetozÃ¡r': 1654, 'verejnosti': 1655, 'neznamenajÃº': 1656, 'pouÄovanie': 1657, 'nechajte': 1658, 'nÃ¡hodou': 1659, 'MFF': 1660, 'korenie': 1661, 'ZaÄÃ­nam': 1662, 'nÃ¡boÅ¾enskÃ½': 1663, 'putovanÃ­': 1664, 'PreÄo': 1665, 'Peter': 1666, 'vÅˆaÅ¥': 1667, 'tej': 1668, 'konaÅ¥': 1669, 'sneh': 1670, 'Foster': 1671, 'vziaÅ¥': 1672, 'neobmedzenÃ½m': 1673, 'VÃ¡m': 1674, 'hlÃ¡ska': 1675, 'Å¡Ã­pok': 1676, 'strÃ½ko': 1677, 'BÃºtoru': 1678, 'zavesila': 1679, 'populizmus': 1680, 'hraÅ¥': 1681, 'malÃº': 1682, 'InÃ½ch': 1683, 'ako': 1684, 'fotoalbumy': 1685, 'nikdy': 1686, 'Karimatky': 1687, 'pretoÅ¾e': 1688, 'spoÄ¾ahlivÃ½': 1689, 'Ä¾udskÃ¡': 1690, 'lepÅ¡ie': 1691, 'tade': 1692, 'Neter': 1693, 'prispelo': 1694, 'Äasovo': 1695, 'kamarÃ¡ta': 1696, 'ZÃ¡horie': 1697, 'nespomenÃºÅ¥': 1698, 'vÃ½hodou': 1699, 'slovenskÃ½': 1700, 'manÅ¾elstva': 1701, 'prevziaÅ¥': 1702, 'poznÃ¡': 1703, 'hru': 1704, 'rÃ´znymi': 1705, 'uchvacujÃºci': 1706, 'Å¡tipendium': 1707, 'Å¡kola': 1708, 'Od': 1709, 'poriadneho': 1710, 'Wojciech': 1711, 'pohybujÃºce': 1712, 'nebezpeÄenstvo': 1713, 'svoj': 1714, 'OdbornÃ¡': 1715, 'SÃº': 1716, 'ma': 1717, 'poznÃ¡m': 1718, 'nevedela': 1719, 'pamiatka': 1720, 'samozrejme': 1721, '(': 1722, 'nadÅ¡enÃ½': 1723, 'najnovÅ¡Ã­': 1724, 'byl': 1725, 'rozhovoriÅ¥': 1726, 'Martina': 1727, 'predstavenie': 1728, 'Po': 1729, 'strÃ¡nku': 1730, 'popise': 1731, 'pokus': 1732, 'Lopez': 1733, 'Armageddon': 1734, 'sa': 1735, 'KvalitnÃ¡': 1736, 'IntenzÃ­vne': 1737, 'PouÅ¾itie': 1738, 'i': 1739, 'podÄ¾a': 1740, 'ÃºÄelovÃº': 1741, 'evanjelickÃ½ch': 1742, 'ÄastÃ­': 1743, 'nepoÄÃ­tam': 1744, 'strÃ¡vil': 1745, 'vo': 1746, 'Dick': 1747, 'Grazie': 1748, 'detskÃ©': 1749, 'AmerickÃ¡': 1750, 'prÃ­klad': 1751, 'ÄeskÃ½ch': 1752, 'daÅ¥': 1753, 'prÃ­ÄetnosÅ¥': 1754, 'Hawke': 1755, 'patrÃ­': 1756, 'vymyslel': 1757, 'Site': 1758, 'mojej': 1759, 'vÃ½kladov': 1760, 'seba': 1761, 'izbovÃ½': 1762, 'teÅ¡Ã­': 1763, 'Hungariae': 1764, 'napadne': 1765, 'Äas': 1766, 'umenÃ­': 1767, 'reÅ¾isÃ©rka': 1768, 'dobrÃ©': 1769, 'chvÃ­li': 1770, 'prÃ­beh': 1771, 'chuÅ¥': 1772, 'naÅ¡ej': 1773, 'Dinah': 1774, 'OdpoveÄ': 1775, 'dÃ­dÅ¾eji': 1776, 'funkÄnÃ½': 1777, 'Ball': 1778, 'ona': 1779, 'znaÄkou': 1780, 'KrÃ¡Ä¾ovnÃ¡': 1781, 'Spustili': 1782, 'ktorÃº': 1783, 'istoty': 1784, 'obloÅ¾te': 1785, 'pomoc': 1786, 'zÃºÄastnenÃ½ch': 1787, 'dlhÃ©': 1788, 'ZlatÃ¡': 1789, 'slobodnÃ½ch': 1790, 'lÃ¡ska': 1791, 'taxikÃ¡ra': 1792, 'Koruna': 1793, 'jedlo': 1794, 'sem': 1795, 'platne': 1796, 'vojne': 1797, 'KompletnÃ½': 1798, 'MaliÄkosÅ¥': 1799, 'mÃ¡m': 1800, 'neobvinili': 1801, 'Miry': 1802, 'PoznÃ¡te': 1803, 'televÃ­zne': 1804, 'Mira': 1805, 'ciele': 1806, 'Å¡tÃºrovcov': 1807, 'mama': 1808, 'InÃ¡': 1809, 'naÅ¡e': 1810, 'drieme': 1811, 'zdravie': 1812, 'otec': 1813, 'scenÃ¡r': 1814, 'orosenÃ½ch': 1815, 'KaÅ¾dÃ©': 1816, 'neposlednom': 1817, 'tajomnÃ­k': 1818, 'inovÃ¡cia': 1819, 'Mostoch': 1820, 'PRÃDE': 1821, 'Duchovny': 1822, 'Okrem': 1823, 'dÃ´leÅ¾itÃ©': 1824, 'praktickÃ©': 1825, 'budÃº': 1826, 'poÄasia': 1827, 'dedina': 1828, 'Äalej': 1829, 'vysokoÅ¡kolÃ¡Äka': 1830, 'rok': 1831, 'hlÃ¡sky': 1832, 'upozorniÅ¥': 1833, 'Potom': 1834, 'festivaly': 1835, 'podobnom': 1836, 'Hookerovi': 1837, 'Jodie': 1838, 'voÄi': 1839, 'poteÅ¡Ã­': 1840, 'nÃ¡vraty': 1841, 'PraÅ¾skÃ©': 1842, 'pochopÃ­': 1843, 'MusÃ­': 1844, 'Ãºmrtia': 1845, 'NedÃ¡vno': 1846, 'Ak': 1847, 'priniesol': 1848, 'nad': 1849, 'svojimi': 1850, 'neÃºplatnÃ½': 1851, 'spoloÄnosti': 1852, 'vÅ¡etkÃ©ho': 1853, 'beletria': 1854, 'udalosti': 1855, 'najlepÅ¡Ã­': 1856, 'pod': 1857, 'protestantov': 1858, 'hranÃ©ho': 1859, 'Viete': 1860, 'zdanlivo': 1861, 'dole': 1862, 'mnoho': 1863, 'seriÃ¡lu': 1864, 'trest': 1865, 'pouÅ¾Ã­vaÅ¥': 1866, 'DeÅ¾a': 1867, 'ktorÃ©mu': 1868, 'dodanÃ­': 1869, 'minÃºt': 1870, 'krÃ¡snych': 1871, 'objem': 1872, 'noÅ¾Ã­k': 1873, 'desiatich': 1874, 'PÃ´jdu': 1875, 'slovenskÃ¡': 1876, 'vÃ½vrtku': 1877, 'doteraz': 1878, 'UpÃ­nacie': 1879, 'Urobila': 1880, 'kinÃ¡ch': 1881, 'akciu': 1882, 'JednoznaÄne': 1883, 'predpokladÃ¡me': 1884, 'spanie': 1885, 'adresa': 1886, 'emigrovanÃ½ch': 1887, 'citlivejÅ¡Ã­ch': 1888, 'istotu': 1889, 'inÃ½m': 1890, 'o': 1891, 'Jaro': 1892, 'typ': 1893, 'jazykovedcovi': 1894, 'Moja': 1895, 'MiloÅ¡': 1896, 'Uhorska': 1897, 'vstupuje': 1898, 'vyhlÃ¡senie': 1899, 'medzinÃ¡rodnej': 1900, 'Madonna': 1901, 'Michal': 1902, 'relÃ¡ciu': 1903, 'cennÃ¡': 1904, 'veci': 1905, 'Ani': 1906, 'suchÃ½': 1907, 'dÃ¡tumy': 1908, 'koniec': 1909, 'sveta': 1910, 'menÃ¡': 1911, 'medze': 1912, 'polhodinu': 1913, 'odpovede': 1914, 'Thurman': 1915, 'Zoberiem': 1916, 'Niekto': 1917, 'takej': 1918, 'blbec': 1919, 'letnÃ©ho': 1920, 'HlavnÃº': 1921, 'plÃ¡novanÃ¡': 1922, 'PokraÄovala': 1923, 'Ryana': 1924, 'slovenskÃ½ch': 1925, 'nakrÃºtenÃ½': 1926, 'svojÃ­m': 1927, 'baterky': 1928, 'film': 1929, 'boli': 1930, 'nacionalisticko': 1931, 'priamo': 1932, 'skÃ´r': 1933, 'norma': 1934, 'skutoÄnej': 1935, 'Has': 1936, 'cenu': 1937, 'Skalici': 1938, 'katalÃ³gu': 1939, 'koni': 1940, 'zbytoÄnÃ©': 1941, 'vÃ­kend': 1942, '20': 1943, 'kompozÃ­cie': 1944, 'rockovÃ½': 1945, 'uvedenÃ­': 1946, 'zastihla': 1947, 'Ale': 1948, 'nÃ¡padov': 1949, 'stali': 1950, 'Äloveku': 1951, 'Autobusy': 1952, 'otÃ¡zka': 1953, 'rozpisovaÅ¥': 1954, 'odovzdal': 1955, 'Äo': 1956, 'filmovÃ½ch': 1957, 'backgroundom': 1958, '.': 1959, 'grafickÃº': 1960, 'vidÃ­': 1961, 'aspoÅˆ': 1962, 'irÃ³nia': 1963, 'slovenskÃ©': 1964, 'energiou': 1965, 'BohuÅ¾iaÄ¾': 1966, 'Jedlo': 1967, 'program': 1968, 'moÅ¾nosti': 1969, 'novinke': 1970, 'KapsiÄka': 1971, 'Jednoducho': 1972, 'na': 1973, 'oboch': 1974, 'ktorÃ©': 1975, 'zavolaÅ¥': 1976, 'Flora': 1977, 'ide': 1978, 'Å tefana': 1979, 'negÃ¡cia': 1980, 'tam': 1981, 'povedal': 1982, 'hneÄ': 1983, 'MyslÃ­m': 1984, 'pravdivosti': 1985, 'stole': 1986, 'nedeÄ¾u': 1987, 'Hedy': 1988, 'veÄerov': 1989, 'potrestanÃ½': 1990}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = data.load_vocabulary()\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this vocabulary, we can easily transform any text into a list of ids:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kde', 'tÃ¡', 'Ä¾udskÃ¡', 'duÅ¡a', 'drieme', '?') [1343, 408, 1690, 70, 1811, 1460]\n"
     ]
    }
   ],
   "source": [
    "sentence = dataset[0].text\n",
    "ids = [vocabulary[word] for word in sentence]\n",
    "print(sentence, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence is then simply a sequence of integers. We feed this sequence into neural networks via a so called _embedding layer_. For a vocabulary of size $V$, such layer contains an _embedding matrix_ $\\mathbf{E}$ with $V$ rows. In this matrix, $i$-th row is a vector representation for word with `id == i`. When we have a vocabulary with 500 word, $\\mathbf{E}$ will have 500 rows and in each row a vector representation for one word will be stored. The number of columns - how long are the word representations - is a hyperparameter. It is usually in the order of hundreds.\n",
    "\n",
    "Below is how embedding layer for vocabulary with $V = 5$ and embedding size $3$ works on a sequence of integers $\\mathbf{s}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{E} = \\begin{vmatrix}\n",
    "e_{11} & e_{12} & e_{13} \\\\\n",
    "e_{21} & e_{22} & e_{23} \\\\\n",
    "e_{31} & e_{32} & e_{33} \\\\\n",
    "e_{41} & e_{42} & e_{43} \\\\\n",
    "e_{51} & e_{52} & e_{53}\n",
    "\\end{vmatrix}\n",
    "\\quad\n",
    "\\mathbf{s} = \\langle 1, 3, 5, 1 \\rangle\n",
    "\\quad\n",
    "emb_{\\mathbf{E}}(\\mathbf{s}) = \\begin{vmatrix}\n",
    "e_{11} & e_{12} & e_{13} \\\\\n",
    "e_{31} & e_{32} & e_{33} \\\\\n",
    "e_{51} & e_{52} & e_{53} \\\\\n",
    "e_{11} & e_{12} & e_{13} \\\\\n",
    "\\end{vmatrix} \\\\\n",
    "$$\n",
    "\n",
    "The embedding layer is implemented in `keras`. The output from an embedding layer can then be directly fed into a recurrent layer. The code below shows an example of a simple LSTM-based text classificator:\n",
    "\n",
    "```python\n",
    "class TextClassificator(keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, lstm_units. num_classes):   \n",
    "        super(TextClassificator, self).__init__()\n",
    "        \n",
    "        self.emb = Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size)\n",
    "        \n",
    "        self.lstm =LSTM(\n",
    "            units=lstm_units)\n",
    "        \n",
    "        self.dense = Dense(\n",
    "            units=num_classes,\n",
    "            activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "__Exercise 8.4:__ What is the shape of `x` for each step of the `call` method? Check the code for sine wave prediction above for some hints.\n",
    "\n",
    "### Pre-trained embeddings\n",
    "\n",
    "Each row of $\\mathbf{E}$ is a vector representationf for one word. It encodes the semantic information about the word. Because many NLP tasks need the same information, we can actually reuse the trained $\\mathbf{E}$ in other models. This is a type of _transfer learning_. If we store the trained $\\mathbf{E}$, we can then later simply initialize the embedding with this matrix in other models:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "matrix = np.array([...])  # 2D numpy array\n",
    "\n",
    "self.emb = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    embeddings_initializer=Constant(matrix))\n",
    "```\n",
    "\n",
    "When we reuse pre-trained embeddings, we need to make sure that the vocabularies match, i.e. that the ids of the words are the same. If the model that trained the embeddings had the word _dog_ with `id == 1`, the model that uses the emebddings should also have `id == 1` for _dog_.\n",
    "\n",
    "This technique of reusing embeddings is especially useful when we do not have much training data. The pre-trained embeddings provide information that can bootstrap the model. E.g. normally, if the model does not encounter the word _cat_ during the training, it does not know what to do with this word during the evaluation. However, with pre-trained embeddings, the model already saw similar words, such as _dog_. The model then can assume that the words should behave similarly.\n",
    "\n",
    "Often we fix the values of pre-trained embeddings during the training. That means that the value of $\\mathbf{E}$ is not updated by the training algorithm and it will stay the same throughout the whole training. Whether to train the pre-train embedding matrix is also a hyperparameter. It is usually not recommended to train the pre-trained embeddings, unless you have a relatively big dataset. You can control this behavior by using `trainable` parameter of `Embedding` layer:\n",
    "\n",
    "```python\n",
    "self.emb = Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=embedding_size,\n",
    "    embeddings_initializer=Constant(matrix),\n",
    "    trainable=False)\n",
    "```\n",
    "\n",
    "#### Embedding libraries\n",
    "\n",
    "There are multiple libraries used to generate word embeddings that you can use: `word2vec`, `GloVe`, `fastText`, and many others. They train the word representation on language modeling tasks. I would recommend using `fastText`, empirically it has a very good performance.\n",
    "\n",
    "You can pre-train your own embeddings with these libraries by providing them a text corpus. Or even better, you can simply download the pre-trained representations that someone else already created, e.g. `fastText` has [published their representations for many languages](https://fasttext.cc/docs/en/crawl-vectors.html). The pre-trained embeddings are simply a file where in each row we have a word and its vector written. Check out the `/week_8/data/embeddings` file in the repository.\n",
    "\n",
    "__Exercise 8.5:__ TensorBoard can also visualize word embeddings so we can explore them. Check out this [online demo](https://projector.tensorflow.org/) for a visualization of an English vocabulary. I would recommend using _T-SNE_ or _UMAP_ visualization aglorithms (you can pick them in the left menu). We have also prepared a visualization of a limited Slovak dictionary in your local [TensorBoard Projector tab](http://localhost:6006/#projector). There you can check various interesting relations between words, e.g. use the search function on the right to find `1999` word. You should see that it is in a cluster of calendar-related words.\n",
    "\n",
    "__Exercise 8.6:__ Check out this online [demo of word2vec embeddings](http://turbomaze.github.io/word2vecjson/). You can use any English word (as long as it is in the demo vocabulary) as query and it should return a list of the most similar words according to the pre-trained embeddings. Try some words to see what are the results.\n",
    "\n",
    "#### Vocabularies in practice\n",
    "\n",
    "We operate with two vocabularies, a vocabulary of words in our dataset and a vocabulary of words for which we have pre-trained embeddings. These two usually do not overlap perfectly. We have two issues:\n",
    "\n",
    "1. _Useless pre-trained embeddings._ We can have pre-trained representations for words that are not present in the training or testing data. We can simply discard embeddings like these during an experiment.\n",
    "2. _Missing pre-trained embeddings._ We can have words in our data for which we do not have pre-trained embeddings. There are multiple strategies of handling words like these:\n",
    "\n",
    "  - First, we should check for pre-trained embeddings for similar words. Maybe we need to lowecase the word or add/remove diacritics.\n",
    "  - We can create a random vector for words like these.\n",
    "  - We can discard such words and use a special `<unknown>` token instead of them. This token can simply have a zero vector as an embedding.\n",
    "\n",
    "### Programming Assignment 8.7: Part-of-Speech Tagger [2 pts]\n",
    "\n",
    "Part-of-speech tagging _(POS tagging)_ is a classical NLP task. We want to mark each word in a sentence with a correct POS tag. POS tags are grammatical categories of words, such as _verb_, _noun_, etc. The data for this task consist of sentences and their respective POS tags:\n",
    "\n",
    "```\n",
    "Kde     ADV\n",
    "tÃ¡      DET\n",
    "Ä¾udskÃ¡  ADJ\n",
    "duÅ¡a    NOUN\n",
    "drieme  VERB\n",
    "?       PUNCT\n",
    "```\n",
    "\n",
    "In this case we use the [Universal POS tagset](https://universaldependencies.org/u/pos/) designed to handle majority of natural languages. Check the link to see how are the tags defined. In this task, we are essentially trying to perform a classification for each word. We can solve this task with an RNN architecture:\n",
    "\n",
    "1. We feed the network with word ids.\n",
    "2. We use an embedding layer to project them into vector space.\n",
    "3. We use _many to many_ RNN to process the vectors.\n",
    "4. We use a softmax dense layer to make a prediction for each word.\n",
    "\n",
    "__Exercise 8.8:__ Why do we need _many to many_ RNN?\n",
    "\n",
    "Your task here is to program your own POS tagger. We have prepared two sets of data `data/train` for training and `data/test` for testing. You can use the data loader and vocabulary loader you have seen above. We also prepared a set of pre-trained embeddings in `data/embeddings`. In this case we have a well behaved vocabularies -- all the words from datasets have their embeddings and all the embeddings are used.\n",
    "\n",
    "1. Prepare the data into a dataset that can be fed into a TensorFlow model. Both input data `x` and labels `y` should be an integer 2D matrix with `(num_samples, sentence_length)` shape. Each row is a sequence of ids for one sentence. Sentences can have different lengths - pad the shorter sequences with zeros. The `sentence_length` should be as long as the longest sentence in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kde', 'tÃ¡', 'Ä¾udskÃ¡', 'duÅ¡a', 'drieme', '?') [1343, 408, 1690, 70, 1811, 1460]\n",
      "('ADV', 'DET', 'ADJ', 'NOUN', 'VERB', 'PUNCT') [2, 5, 0, 7, 15, 12]\n",
      "(200, 34) (200, 34)\n",
      "(200, 36) (200, 36)\n"
     ]
    }
   ],
   "source": [
    "train = data.load_pos_data('data/train')\n",
    "test = data.load_pos_data('data/test')\n",
    "vocab = data.load_vocabulary()\n",
    "pos_vocab = data.pos_vocabulary\n",
    "\n",
    "import numpy as np\n",
    "from week_8.backstage.data import *\n",
    "\n",
    "## Hint\n",
    "sample = train[0]\n",
    "word_ids = [vocab[word] for word in sample.text]\n",
    "tag_ids = [pos_vocab[tag] for tag in sample.labels]\n",
    "\n",
    "print(sample.text, word_ids)\n",
    "print(sample.labels, tag_ids)\n",
    "\n",
    "train_x = [[vocab[w] for w in s.text] for s in train]\n",
    "train_y = [[pos_vocab[w] for w in s.labels] for s in train]\n",
    "test_x = [[vocab[w] for w in s.text] for s in test]\n",
    "test_y = [[pos_vocab[w] for w in s.labels] for s in test]\n",
    "\n",
    "\n",
    "train_x = keras.preprocessing.sequence.pad_sequences(train_x, padding='post')\n",
    "train_y = keras.preprocessing.sequence.pad_sequences(train_y, padding='post')\n",
    "test_x = keras.preprocessing.sequence.pad_sequences(test_x, padding='post')\n",
    "test_y = keras.preprocessing.sequence.pad_sequences(test_y, padding='post')\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Implement the described architecture in `keras`. There are some tricky parts to it. First, you should _mask_ the padding you added. Check out the [keras tutorial about masking.](https://www.tensorflow.org/guide/keras/masking_and_padding). Second, you should use bi-directional LSTM. Luckily, `keras` has a `Bidirectional` wrapper. Third, by default the LSTM return only a last output (many to one), here we want the output for all the words (many to many). Check the documentation of LSTM layer.\n",
    "\n",
    "3. If you want to use pre-trained embeddings, we have prepared the pre-trained fastText matrix $\\mathbf{E}$ for you: `data.embedding_matrix(vocabulary)`. You can initialize your embedding layer with it.\n",
    "\n",
    "4. Train the model. Compare the results with and without pre-trained embeddings. Compare the results with bi-directional LSTM and with one-directional LSTM. Compare the results when you train the pre-trained embeddings and when you do not train them. You should be able to get up to 94.5% validation accuracy with these experiments with the default hyperparameters provided in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "200/200 [==============================] - 7s 37ms/sample - loss: 1.8117 - accuracy: 0.2594 - val_loss: 1.4470 - val_accuracy: 0.4391\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 1.7662 - accuracy: 0.7786 - val_loss: 1.4091 - val_accuracy: 0.5885\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 3s 13ms/sample - loss: 1.7070 - accuracy: 0.8885 - val_loss: 1.3575 - val_accuracy: 0.6179\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 3s 13ms/sample - loss: 1.6159 - accuracy: 0.8850 - val_loss: 1.2707 - val_accuracy: 0.5543\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 3s 13ms/sample - loss: 1.4519 - accuracy: 0.6204 - val_loss: 1.1481 - val_accuracy: 0.3293\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 1.2709 - accuracy: 0.3903 - val_loss: 1.0725 - val_accuracy: 0.3353\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 1.1305 - accuracy: 0.4114 - val_loss: 1.0266 - val_accuracy: 0.3671\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 1.0013 - accuracy: 0.3898 - val_loss: 0.9908 - val_accuracy: 0.3719\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.8792 - accuracy: 0.4027 - val_loss: 0.9635 - val_accuracy: 0.4103\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.7694 - accuracy: 0.4453 - val_loss: 0.9397 - val_accuracy: 0.4337\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.6815 - accuracy: 0.4705 - val_loss: 0.9160 - val_accuracy: 0.4439\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.6068 - accuracy: 0.5080 - val_loss: 0.8874 - val_accuracy: 0.4721\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.5284 - accuracy: 0.5660 - val_loss: 0.8532 - val_accuracy: 0.5177\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 3s 13ms/sample - loss: 0.4521 - accuracy: 0.6513 - val_loss: 0.8237 - val_accuracy: 0.5495\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.3840 - accuracy: 0.7067 - val_loss: 0.8004 - val_accuracy: 0.5813\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.3250 - accuracy: 0.7750 - val_loss: 0.7830 - val_accuracy: 0.6095\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.2728 - accuracy: 0.8331 - val_loss: 0.7694 - val_accuracy: 0.6305\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.2263 - accuracy: 0.8834 - val_loss: 0.7581 - val_accuracy: 0.6455\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.1857 - accuracy: 0.9199 - val_loss: 0.7498 - val_accuracy: 0.6533\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.1504 - accuracy: 0.9409 - val_loss: 0.7425 - val_accuracy: 0.6635\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.1212 - accuracy: 0.9640 - val_loss: 0.7376 - val_accuracy: 0.6647\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.0992 - accuracy: 0.9779 - val_loss: 0.7335 - val_accuracy: 0.6635\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.0811 - accuracy: 0.9851 - val_loss: 0.7302 - val_accuracy: 0.6659\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.0675 - accuracy: 0.9866 - val_loss: 0.7278 - val_accuracy: 0.6677\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0572 - accuracy: 0.9872 - val_loss: 0.7257 - val_accuracy: 0.6695\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0491 - accuracy: 0.9877 - val_loss: 0.7244 - val_accuracy: 0.6701\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.0429 - accuracy: 0.9882 - val_loss: 0.7232 - val_accuracy: 0.6695\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.0379 - accuracy: 0.9882 - val_loss: 0.7222 - val_accuracy: 0.6707\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.0339 - accuracy: 0.9882 - val_loss: 0.7216 - val_accuracy: 0.6689\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.0307 - accuracy: 0.9877 - val_loss: 0.7211 - val_accuracy: 0.6713\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 3s 13ms/sample - loss: 0.0280 - accuracy: 0.9892 - val_loss: 0.7207 - val_accuracy: 0.6719\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0260 - accuracy: 0.9892 - val_loss: 0.7202 - val_accuracy: 0.6719\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.0241 - accuracy: 0.9887 - val_loss: 0.7199 - val_accuracy: 0.6725\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0227 - accuracy: 0.9892 - val_loss: 0.7198 - val_accuracy: 0.6713\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 3s 15ms/sample - loss: 0.0214 - accuracy: 0.9897 - val_loss: 0.7196 - val_accuracy: 0.6719\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0204 - accuracy: 0.9897 - val_loss: 0.7194 - val_accuracy: 0.6707\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0195 - accuracy: 0.9897 - val_loss: 0.7194 - val_accuracy: 0.6713\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.0188 - accuracy: 0.9897 - val_loss: 0.7193 - val_accuracy: 0.6701\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 4s 18ms/sample - loss: 0.0181 - accuracy: 0.9892 - val_loss: 0.7195 - val_accuracy: 0.6725\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.0174 - accuracy: 0.9892 - val_loss: 0.7196 - val_accuracy: 0.6701\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.0169 - accuracy: 0.9892 - val_loss: 0.7197 - val_accuracy: 0.6701\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.0164 - accuracy: 0.9897 - val_loss: 0.7198 - val_accuracy: 0.6695\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0158 - accuracy: 0.9897 - val_loss: 0.7199 - val_accuracy: 0.6719\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0154 - accuracy: 0.9892 - val_loss: 0.7199 - val_accuracy: 0.6695\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 3s 16ms/sample - loss: 0.0151 - accuracy: 0.9897 - val_loss: 0.7201 - val_accuracy: 0.6695\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 3s 14ms/sample - loss: 0.0147 - accuracy: 0.9892\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b9becacf7b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m )\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                       total_epochs=1)\n\u001b[0m\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m    372\u001b[0m                                  prefix='val_')\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from datetime import datetime\n",
    "\n",
    "class POSTagger(keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(POSTagger, self).__init__()\n",
    "        self.emb = Embedding(\n",
    "            input_dim=len(vocab), \n",
    "            output_dim=300, \n",
    "            mask_zero=True,\n",
    "            weights=[data.embedding_matrix(vocab)]\n",
    "        )\n",
    "        self.lstm = Bidirectional(LSTM(300, return_sequences=True, activation='softmax'))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.emb(inputs)\n",
    "        mask = self.emb.compute_mask(inputs)\n",
    "        output = self.lstm(x, mask=mask)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = POSTagger()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "\n",
    "logdir = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "model.fit(\n",
    "    x=train_x,\n",
    "    y=train_y,\n",
    "    batch_size=10,\n",
    "    epochs=100,\n",
    "    validation_data=(test_x, test_y),\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c2c81ce83865bf58\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c2c81ce83865bf58\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "The data come from [SlovenskÃ½ NÃ¡rodnÃ½ Korpus (Slovak National Corpus) dataset](https://github.com/UniversalDependencies/UD_Slovak-SNK). We used only a limited amount of 200 sentences for both training and testing set. The pre-trained embeddings come from the official [fastText repository](). The original embeddings have 2M words in them, here we provide only the embeddings for the words you need (~2000 words).\n",
    "\n",
    "#### Submission\n",
    "\n",
    "Submit the code that prepares the testing and training data and your model code as one `.py` file. Also submit the training logs (appropriately named, e.g. `pretrained_emb_bidirectional`). All the files should be zipped.\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- Chistopher Olah has nice blog posts about both [LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) and [word embeddings](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/).\n",
    "- Chapter 10 from Deep Learning book by Ian Goodfellow et al. provides a great overview of RNNs.\n",
    "\n",
    "## Correct Answers\n",
    "\n",
    "__E 8.1:__ Many-to-one.\n",
    "\n",
    "__E 8.2:__ `batch_size` is 10, as defined in `fit` method call. `time_steps` is 250, as defined during dataset creation. `input_dim` is 1, we only input one value each step. `lstm_dim` is 20, as defined during LSTM layer initialization. `dense_dim` is 1, as defined during Dense layer initialization. We always output one value - a target prediction.\n",
    "\n",
    "__E 8.3:__\n",
    "- _Many to one:_ All forms of text classification, such as sentiment analysis, hate-speech detection, spam detection, document classification. It can be also used for language modeling - predicting the next word in a sentence.\n",
    "- _One to many:_ Various task generation tasks, such as image captioning.\n",
    "- _Many to many:_ Word tagging tasks, such as part-of-speech tagging, named entity recognition, dependency parsing.\n",
    "- _Sequence to sequence:_ Tasks, when we have a sequence both as input and output, e.g. machine translation, speech-to-text, text-to-speech, summarization.\n",
    "\n",
    "__E 8.4:__\n",
    "```python\n",
    "def call(self, x):     # (batch_size, time_steps)\n",
    "    x = self.emb(x)    # (batch_size, time_steps, embedding_size)\n",
    "    x = self.lstm(x)   # (batch_size, lstm_units)\n",
    "    x = self.dense(x)  # (batch_size, num_classes)\n",
    "    return x\n",
    "```\n",
    "\n",
    "__E 8.8:__ Because we need an output for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
