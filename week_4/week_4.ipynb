{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You need to run this cell for the code in following cells to work.\n",
    "\"\"\"\n",
    "\n",
    "# Enable module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "from week_4.backstage.load_data import load_data\n",
    "from week_4.backstage.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "__Goals for this week__\n",
    "\n",
    "We will start working with _TensorFlow_ - a modern deep learning framework. We will implement a multilayer perceptron model and train it with this framework.\n",
    "\n",
    "__Feedback__\n",
    "\n",
    "This lab is a work in progress. If you notice a mistake, notify us or you can even make a pull request. Also please fill the [questionnaire](https://forms.gle/r27nBAvnMC7jbjJ58) after you finish this lab to give us feedback.\n",
    "\n",
    "## Reminder\n",
    "\n",
    "__You submit your project proposal in a week.__\n",
    "\n",
    "## TensorFlow\n",
    "\n",
    "TensorFlow (TF) is a state-of-the-art framework for neural network development, training and deployment. It provides:\n",
    "\n",
    "1. Basic building blocks for models - layers, loss functions, activation functions, etc. The neural models are mostly built from common building blocks.\n",
    "2. Auto-differentiation. We do not have to calculate the derivatives of the loss function w.r.t. parameters. Instead these quantities are derived automatically. The training algorithms, such as SGD, can also be used via handy API.\n",
    "3. Toolset for visualization, deployment, distributed computing, etc.\n",
    "\n",
    "We will use _TensorFlow 2.0_ in our labs.\n",
    "\n",
    "### Tensors\n",
    "\n",
    "`Tensor` is the basic TF type. Constants called by `tf.constant` are immutable, while variables called by `tf.Variable` can be changed later. Therefore variables are used as model parameters. Notice that each tensor has a `shape` and a `dtype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.arange(15, dtype=np.float).reshape(5,3)\n",
    "print(np_array)\n",
    "print()\n",
    "\n",
    "tf_tensor = tf.constant(np_array)\n",
    "print(tf_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the usual matrix operations with these tensors, e.g. $\\sigma(\\mathbf{Wx} + \\mathbf{b})$. Common operators, such as `+` for addition or `@` for matrix multiplication are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "w = tf.constant([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.2, 0.1, 0.3],\n",
    "    [0.3, 0.1, 0.2]\n",
    "])\n",
    "\n",
    "x = tf.constant([\n",
    "    [0.5],\n",
    "    [-0.3],\n",
    "    [0.2],\n",
    "])\n",
    "\n",
    "b = tf.constant([\n",
    "    [0.3],\n",
    "    [-0.4],\n",
    "    [0.2],\n",
    "])\n",
    "\n",
    "print(sigma(w @ x + b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we need to define $\\mathbf{x}$ and $\\mathbf{b}$ as 2D tensors with shape `(3, 1)` if we want them to behave like column vectors.\n",
    "\n",
    "### Defining models\n",
    "\n",
    "Instead of working with these tensors, we will use a high-level API for model definition called `keras`. Within this API we have `tf.keras.Model` class for models. `keras` models are basic computational units that transform input $x$ to output $\\hat{y}$ and that can be trained via SGD or similar algorithms. \n",
    "\n",
    "We will define it using predefined `Layer`s. Compared to `keras` models, layers are more atomic computational units, that can be reused, e.g. `Dense` layer is an implementation of MLP layer equation: $\\sigma(\\mathbf{Wx} + \\mathbf{b})$. Carefully read the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(keras.Model):  # Subclassing\n",
    "    \n",
    "    def __init__(self, dim_output, dim_hidden, num_layers=1, activation=keras.activations.linear):\n",
    "        super(MultilayerPerceptron, self).__init__(name='multilayer_perceptron')\n",
    "        self.dim_output = dim_output\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "        # Within Model.__init__ we initialize all the layers we will use\n",
    "        self.hidden_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layer = keras.layers.Dense(units=dim_hidden, activation=activation)\n",
    "            self.hidden_layers.append(layer)\n",
    "        self.layer_o = keras.layers.Dense(units=dim_output, activation=keras.activations.softmax)\n",
    "\n",
    "    def call(self, x):  # call defines the flow of the computation, e.g. in this particular model\n",
    "                        # we simply call the two layers one after the oter\n",
    "        h = x\n",
    "        for layer in self.hidden_layers:\n",
    "            h = layer(h)\n",
    "        y = self.layer_o(h)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.1:__ Check the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) of `Dense` layer. The arguments of the layer provide us with additional options, you should know at least the first five arguments. Looking at thes arguments, what is missing in the definition used above?\n",
    "\n",
    "### Training models\n",
    "\n",
    "We will train this model to classify the _Iris_ dataset from previous lab. Training models defined like this is really easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('iris.csv', num_classes=3)\n",
    "\n",
    "# Reminder how these data look\n",
    "for x, y in list(zip(data.x, data.y))[:5]:  # First 5 samples\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 0s 3ms/sample - loss: 0.4304 - accuracy: 0.3333\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 354us/sample - loss: 0.4238 - accuracy: 0.3333\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 397us/sample - loss: 0.4160 - accuracy: 0.3333\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 338us/sample - loss: 0.4105 - accuracy: 0.3333\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 340us/sample - loss: 0.4056 - accuracy: 0.3333\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 348us/sample - loss: 0.3981 - accuracy: 0.3200\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 391us/sample - loss: 0.3844 - accuracy: 0.3200\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 333us/sample - loss: 0.3597 - accuracy: 0.2933\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 330us/sample - loss: 0.3326 - accuracy: 0.1267\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 354us/sample - loss: 0.3115 - accuracy: 0.3067\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 331us/sample - loss: 0.2953 - accuracy: 0.3267\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 427us/sample - loss: 0.2790 - accuracy: 0.3333\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 335us/sample - loss: 0.2652 - accuracy: 0.3333\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 299us/sample - loss: 0.2527 - accuracy: 0.3333\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 311us/sample - loss: 0.2370 - accuracy: 0.3000\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 306us/sample - loss: 0.2223 - accuracy: 0.4733\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 340us/sample - loss: 0.2071 - accuracy: 0.3867\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 405us/sample - loss: 0.1941 - accuracy: 0.5733\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 358us/sample - loss: 0.1822 - accuracy: 0.6467\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 345us/sample - loss: 0.1719 - accuracy: 0.6733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff31c78fdd8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultilayerPerceptron(  # We create a new model\n",
    "    dim_output=3,\n",
    "    dim_hidden=32)\n",
    "\n",
    "model.compile(  # By compiling we prepare the model for training\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003),  # We pick a optimizer algorithm\n",
    "    loss='mean_squared_error',  # We pick a loss function\n",
    "    metrics=['accuracy'])  # We pick evaluation metrics\n",
    "\n",
    "model.fit(  # Fit runs the training over provided data\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the selling point for using modern neural frameworks. The model is trained via SGD, but we do not need to calculate derivatives. Instead they are calculated automatically by TF. We also do not need to program how SGD works, nor we need to define the loss functions or metrics.\n",
    "\n",
    "All that we done manually last week is now hidden behind the `fit` function. You should already be familiar with all the concepts that were introduced in the code above, such as `epochs`, `batch_size`, `metrics`, `loss`, `optimizer`, etc.\n",
    "\n",
    "### Programming assignment 4.2: Multilayer Perceptron [1pt]\n",
    "\n",
    "Extend the `MultilayerPerceptron` definition above so that we can have model:\n",
    "\n",
    "1. _Arbitrary number of layers:_ `num_layers=1` tells us that we have one hidden layer in our model.\n",
    "2. _Arbitrary activation functions:_ Function is passed as an argument in this case.\n",
    "\n",
    "Check the first command below to see how is the new object created. You can play around with various hyperparameters to see how do the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 0s 3ms/sample - loss: 0.2663 - accuracy: 0.3333\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 0s 426us/sample - loss: 0.2544 - accuracy: 0.3333\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 0s 424us/sample - loss: 0.2484 - accuracy: 0.3333\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 0s 435us/sample - loss: 0.2444 - accuracy: 0.3333\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 0s 355us/sample - loss: 0.2409 - accuracy: 0.3200\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 0s 435us/sample - loss: 0.2377 - accuracy: 0.3333\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 0s 413us/sample - loss: 0.2341 - accuracy: 0.3467\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 0s 383us/sample - loss: 0.2314 - accuracy: 0.3400\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 0s 389us/sample - loss: 0.2291 - accuracy: 0.3467\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 0s 373us/sample - loss: 0.2271 - accuracy: 0.3533\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 0s 327us/sample - loss: 0.2257 - accuracy: 0.3800\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 0s 376us/sample - loss: 0.2245 - accuracy: 0.3800\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 0s 366us/sample - loss: 0.2239 - accuracy: 0.3400\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 0s 389us/sample - loss: 0.2234 - accuracy: 0.3933\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 0s 326us/sample - loss: 0.2231 - accuracy: 0.3667\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 0s 362us/sample - loss: 0.2229 - accuracy: 0.3667\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 0s 352us/sample - loss: 0.2228 - accuracy: 0.3933\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 0s 349us/sample - loss: 0.2227 - accuracy: 0.3400\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 0s 369us/sample - loss: 0.2227 - accuracy: 0.3533\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 0s 374us/sample - loss: 0.2225 - accuracy: 0.3933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff31c0fdc18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32,\n",
    "    num_layers=3,\n",
    "    activation=keras.activations.sigmoid)\n",
    "\n",
    "# compile and fit are the same as above\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks\n",
    "\n",
    "Apart from automatic training, TF also provides us with a lot of pre-programmed parts, such as:\n",
    "\n",
    "- [Loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses), such as mean squared error.\n",
    "- [Activation functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations), such as sigmoid or ReLU.\n",
    "- [Optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), such as SGD.\n",
    "- [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics), such as accuracy.\n",
    "- [Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers), such as Dense layer, which is basically the MLP layer $\\sigma(\\mathbf{Wx} + \\mathbf{b})$.\n",
    "- [Initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers), such as Glorot (Xavier) initialization.\n",
    "- and many other features.\n",
    "\n",
    "You have seen an example of each of these parts in the code above. E.g. we can use `loss='mean_squared_error'` because a loss function with such name is defined in `keras.losses`. You should be able to program most of your projects with these pre-programmed building blocks. But you can of course define your own blocks by following the documentation.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "`fit` also has a support for creating a test set, called `validation set` in TF. By using `validation_split` in `fit` it uses part of the data for evaluation. This is the same concept we practiced last week. Check the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) of `fit` to see what other options it has - you should understand most of them already. Run the following code to see how it looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 0s 3ms/sample - loss: 0.2453 - accuracy: 0.3417 - val_loss: 0.2478 - val_accuracy: 0.3000\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 608us/sample - loss: 0.2439 - accuracy: 0.3417 - val_loss: 0.2460 - val_accuracy: 0.3000\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 561us/sample - loss: 0.2426 - accuracy: 0.3417 - val_loss: 0.2445 - val_accuracy: 0.3000\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 510us/sample - loss: 0.2414 - accuracy: 0.3417 - val_loss: 0.2430 - val_accuracy: 0.3000\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 522us/sample - loss: 0.2403 - accuracy: 0.3417 - val_loss: 0.2417 - val_accuracy: 0.3000\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 491us/sample - loss: 0.2393 - accuracy: 0.3417 - val_loss: 0.2405 - val_accuracy: 0.3000\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 554us/sample - loss: 0.2382 - accuracy: 0.3417 - val_loss: 0.2393 - val_accuracy: 0.3000\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 599us/sample - loss: 0.2373 - accuracy: 0.3417 - val_loss: 0.2382 - val_accuracy: 0.3000\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 687us/sample - loss: 0.2364 - accuracy: 0.3417 - val_loss: 0.2371 - val_accuracy: 0.3000\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 558us/sample - loss: 0.2356 - accuracy: 0.3417 - val_loss: 0.2362 - val_accuracy: 0.3000\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 509us/sample - loss: 0.2347 - accuracy: 0.3417 - val_loss: 0.2352 - val_accuracy: 0.3000\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 555us/sample - loss: 0.2340 - accuracy: 0.3417 - val_loss: 0.2343 - val_accuracy: 0.3000\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 544us/sample - loss: 0.2331 - accuracy: 0.3417 - val_loss: 0.2335 - val_accuracy: 0.3000\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 509us/sample - loss: 0.2323 - accuracy: 0.3417 - val_loss: 0.2326 - val_accuracy: 0.3000\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 516us/sample - loss: 0.2316 - accuracy: 0.3417 - val_loss: 0.2318 - val_accuracy: 0.3000\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 511us/sample - loss: 0.2309 - accuracy: 0.3417 - val_loss: 0.2311 - val_accuracy: 0.3000\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 555us/sample - loss: 0.2302 - accuracy: 0.3417 - val_loss: 0.2304 - val_accuracy: 0.3000\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 512us/sample - loss: 0.2296 - accuracy: 0.3417 - val_loss: 0.2297 - val_accuracy: 0.3000\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 527us/sample - loss: 0.2289 - accuracy: 0.3417 - val_loss: 0.2290 - val_accuracy: 0.3000\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 545us/sample - loss: 0.2283 - accuracy: 0.3417 - val_loss: 0.2284 - val_accuracy: 0.3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2f43783c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32,\n",
    "    num_layers=3,\n",
    "    activation=keras.activations.sigmoid)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20,\n",
    "    validation_split=0.2)  # This was added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "_TensorBoard_ (TB) is a great visualization tool for training TF models. First, we need to tell the model that it should create TB-related logs during training. The easiest way is to use callbacks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff2b96aa470>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32,\n",
    "    num_layers=3,\n",
    "    activation=keras.activations.sigmoid)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=os.path.join(\"logs\", timestamp()),\n",
    "    histogram_freq=1)\n",
    "\n",
    "model.fit(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tensorboard_callback],  # Callback\n",
    "    verbose=0)  # Supressing text output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visulize the results with TensorBoard. We can start it from terminal, or directly from jupyter (run the next cell). After you start TB, you can access it from the notebook, or more conveniently directly from your browser at http://localhost:6006.\n",
    "\n",
    "The first two tabs _Scalars_ and _Graphs_ are the most interesting to you right now. The first shows how do various quantities change during the training. It shows them for both train and validation data. You can also see the results for multiple runs at the same time. Run the training above again, but change some hyperparameters, e.g. learning rate. Then you can directly compare the results in TB.\n",
    "\n",
    "_Graphs_ show a graph of your model, i.e. how does it compute the results. By double-clicking you can open individual parts and see how are they defined, i.e. open your model and then a dense layer within to see how is it defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 61), started 1:05:10 ago. (Use '!kill 61' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2dfb7c3f14ddb3b9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2dfb7c3f14ddb3b9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming assignment 4.3: Multilayer Perceptron, Part 2 [1pt]\n",
    "\n",
    "First, run the current model with TB callback to get some results. Then implement the following changes:\n",
    "\n",
    "1. Use _softmax_ as an activation function, but for the __last__ layer. Use the activation provided by `activation` argument for all the other - hidden - layers.\n",
    "2. Use _categorical crossentropy_ as a loss function.\n",
    "\n",
    "You can find both these functions in the links of pre-programmed building blocks above. Then compare the results of this new implementation in TensorBoard.\n",
    "\n",
    "#### Submission\n",
    "\n",
    "Save the code for your `MultilayerPerceptron` and you training commands (e.g. `compile`, `fit`, etc.) in a `mlp.py` file and submit them to AIS. You need to complete __PA 4.2__ before proceeding to __PA 4.3__. There are no tests this week so consult the submission with your teacher if needed.\n",
    "\n",
    "## Gradient Tape\n",
    "\n",
    "`fit` is a very convenient way of training neural models, but sometimes we need more flexibility and control. For example, with `fit` we can not track the training step by step (e.g. for debugging). The model is compiled into a computation graph in the background. So if you want to have a debugging print within a model, it will not run. E.g., try printing the value of `h` in the model `call`.\n",
    "\n",
    "Instead we can use so called `GradientType`. With this tape the debugging print of `h` will run. Check the following code, it is very similar in how we defined SGD in previous labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer multilayer_perceptron is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 0 Loss: 0.3324832\n",
      "Epoch: 1 Loss: 0.2617142\n",
      "Epoch: 2 Loss: 0.16195057\n",
      "Epoch: 3 Loss: 0.13432351\n",
      "Epoch: 4 Loss: 0.12554379\n",
      "Epoch: 5 Loss: 0.119047165\n",
      "Epoch: 6 Loss: 0.114001796\n",
      "Epoch: 7 Loss: 0.10989445\n",
      "Epoch: 8 Loss: 0.10641589\n",
      "Epoch: 9 Loss: 0.10337446\n",
      "Epoch: 10 Loss: 0.100647815\n",
      "Epoch: 11 Loss: 0.09815552\n",
      "Epoch: 12 Loss: 0.09584325\n",
      "Epoch: 13 Loss: 0.09367351\n",
      "Epoch: 14 Loss: 0.091619775\n",
      "Epoch: 15 Loss: 0.08966307\n",
      "Epoch: 16 Loss: 0.0877894\n",
      "Epoch: 17 Loss: 0.08598843\n",
      "Epoch: 18 Loss: 0.08425234\n",
      "Epoch: 19 Loss: 0.082575135\n",
      "Epoch: 20 Loss: 0.08095212\n",
      "Epoch: 21 Loss: 0.07937958\n",
      "Epoch: 22 Loss: 0.07785457\n",
      "Epoch: 23 Loss: 0.07637461\n",
      "Epoch: 24 Loss: 0.07493768\n",
      "Epoch: 25 Loss: 0.073542036\n",
      "Epoch: 26 Loss: 0.07218617\n",
      "Epoch: 27 Loss: 0.07086872\n",
      "Epoch: 28 Loss: 0.0695885\n",
      "Epoch: 29 Loss: 0.06834439\n",
      "Epoch: 30 Loss: 0.067135386\n",
      "Epoch: 31 Loss: 0.06596047\n",
      "Epoch: 32 Loss: 0.06481878\n",
      "Epoch: 33 Loss: 0.06370938\n",
      "Epoch: 34 Loss: 0.06263143\n",
      "Epoch: 35 Loss: 0.061584115\n",
      "Epoch: 36 Loss: 0.06056659\n",
      "Epoch: 37 Loss: 0.059578076\n",
      "Epoch: 38 Loss: 0.058617774\n",
      "Epoch: 39 Loss: 0.057684924\n",
      "Epoch: 40 Loss: 0.056778755\n",
      "Epoch: 41 Loss: 0.055898547\n",
      "Epoch: 42 Loss: 0.055043556\n",
      "Epoch: 43 Loss: 0.054213047\n",
      "Epoch: 44 Loss: 0.05340632\n",
      "Epoch: 45 Loss: 0.052622695\n",
      "Epoch: 46 Loss: 0.051861465\n",
      "Epoch: 47 Loss: 0.05112198\n",
      "Epoch: 48 Loss: 0.05040356\n",
      "Epoch: 49 Loss: 0.04970558\n",
      "Epoch: 50 Loss: 0.04902741\n",
      "Epoch: 51 Loss: 0.04836844\n",
      "Epoch: 52 Loss: 0.04772806\n",
      "Epoch: 53 Loss: 0.047105692\n",
      "Epoch: 54 Loss: 0.04650077\n",
      "Epoch: 55 Loss: 0.045912728\n",
      "Epoch: 56 Loss: 0.045341033\n",
      "Epoch: 57 Loss: 0.044785164\n",
      "Epoch: 58 Loss: 0.044244602\n",
      "Epoch: 59 Loss: 0.04371887\n",
      "Epoch: 60 Loss: 0.043207463\n",
      "Epoch: 61 Loss: 0.042709935\n",
      "Epoch: 62 Loss: 0.042225827\n",
      "Epoch: 63 Loss: 0.041754697\n",
      "Epoch: 64 Loss: 0.041296124\n",
      "Epoch: 65 Loss: 0.04084971\n",
      "Epoch: 66 Loss: 0.04041504\n",
      "Epoch: 67 Loss: 0.039991736\n",
      "Epoch: 68 Loss: 0.03957945\n",
      "Epoch: 69 Loss: 0.039177798\n",
      "Epoch: 70 Loss: 0.038786437\n",
      "Epoch: 71 Loss: 0.038405042\n",
      "Epoch: 72 Loss: 0.038033288\n",
      "Epoch: 73 Loss: 0.037670847\n",
      "Epoch: 74 Loss: 0.03731745\n",
      "Epoch: 75 Loss: 0.03697278\n",
      "Epoch: 76 Loss: 0.036636557\n",
      "Epoch: 77 Loss: 0.036308534\n",
      "Epoch: 78 Loss: 0.03598844\n",
      "Epoch: 79 Loss: 0.035676006\n",
      "Epoch: 80 Loss: 0.03537102\n",
      "Epoch: 81 Loss: 0.03507322\n",
      "Epoch: 82 Loss: 0.0347824\n",
      "Epoch: 83 Loss: 0.034498353\n",
      "Epoch: 84 Loss: 0.034220822\n",
      "Epoch: 85 Loss: 0.03394966\n",
      "Epoch: 86 Loss: 0.033684634\n",
      "Epoch: 87 Loss: 0.03342557\n",
      "Epoch: 88 Loss: 0.03317229\n",
      "Epoch: 89 Loss: 0.032924607\n",
      "Epoch: 90 Loss: 0.032682367\n",
      "Epoch: 91 Loss: 0.032445405\n",
      "Epoch: 92 Loss: 0.03221356\n",
      "Epoch: 93 Loss: 0.03198667\n",
      "Epoch: 94 Loss: 0.031764615\n",
      "Epoch: 95 Loss: 0.031547222\n",
      "Epoch: 96 Loss: 0.031334385\n",
      "Epoch: 97 Loss: 0.031125959\n",
      "Epoch: 98 Loss: 0.030921815\n",
      "Epoch: 99 Loss: 0.030721826\n"
     ]
    }
   ],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "\n",
    "# loss_function = keras.losses.CategoricalCrossentropy()\n",
    "# You can use cross-entropy loss if you completed PA 4.3\n",
    "    \n",
    "def step(xs, ys):  # This has the same meaning as step function in previous labs\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(xs)  # Model predictions\n",
    "        loss = loss_function(ys, preds)  # The value of loss function comparing the true\n",
    "                                         # values ys with predictions\n",
    "\n",
    "    gradient = tape.gradient(\n",
    "        target=loss,\n",
    "        sources=model.trainable_variables)  # Calculate the gradient of loss function w.r.t. model parameters.\n",
    "                                            # This behaves the same as gradient methods from previous labs.\n",
    "        \n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))  # Applies the computed gradient on current\n",
    "                                                                         # parameter values.\n",
    "    \n",
    "def loss(xs, ys):\n",
    "    preds = model(xs)\n",
    "    return loss_function(ys, preds)\n",
    "    \n",
    "num_epochs = 100\n",
    "batch_size = 5\n",
    "num_samples = len(data.x)\n",
    "\n",
    "# Training loop (without shuffling for simplicity)\n",
    "for e in range(num_epochs):\n",
    "    for i in np.arange(0, num_samples, batch_size):  # Batching\n",
    "        step(data.x[i:i+batch_size], data.y[i:i+batch_size])\n",
    "    print('Epoch:', e, 'Loss:', loss(data.x, data.y).numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "Check out TF [Tutorials](https://www.tensorflow.org/tutorials) and [Guide](https://www.tensorflow.org/guide) for some further reading. Note that all the documents there are in fact Jupyter notebooks, so you can download them and run them here. At this point, you should check:\n",
    "\n",
    "- _Tutorials > ML basics with Keras_ - for some basic practical use cases.\n",
    "- _Tutorials > Load and preproccess data_ - to see how can you load data using TF.\n",
    "\n",
    "You can also check:\n",
    "\n",
    "- _Guide > Keras_ - for more in-depth explanation of how TF works.\n",
    "\n",
    "This [notebook](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO) is also a nice showcase of additional TF features.\n",
    "\n",
    "## Correct Answers\n",
    "\n",
    "__E 4.1:__ Activation function is missing. `Dense` uses linear activation by default. We need to use the `activation` argument to add an activation function. There we can use our own activation functions, such as `sigma` defined previously. Or, we can use some of the pre-programmed activation functions from `tf.keras.activations` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
