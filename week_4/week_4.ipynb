{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You need to run this cell for the code in following cells to work.\n",
    "\"\"\"\n",
    "\n",
    "# Enable module reloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "from week_4.backstage.load_data import load_data\n",
    "from week_4.backstage.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "__Goals for this week__\n",
    "\n",
    "We start working with _TensorFlow_ - a modern deep learning framework. We will implement a multilayer perceptron model and train it.\n",
    "\n",
    "__Feedback__\n",
    "\n",
    "This lab is a work in progress. If you notice a mistake, notify us or you can even make a pull request. Also please fill the [questionnaire](https://forms.gle/r27nBAvnMC7jbjJ58) after you finish this lab to give us feedback.\n",
    "\n",
    "## Reminder\n",
    "\n",
    "__You submit your project proposal in a week.__\n",
    "\n",
    "## TensorFlow\n",
    "\n",
    "TensorFlow (TF) is a state-of-the-art framework for neural network development, training and deployment. It provides:\n",
    "\n",
    "1. Basic building blocks for models - layers, loss functions, activation functions, etc. Most of the neural models are built from very common building blocks.\n",
    "2. Auto-differentiation. We do not have to calculate the derivatives of the loss function w.r.t. parameters. Instead these quantities are derived automatically. The training algorithms, such as SGD, can also be used via handy API.\n",
    "3. Toolset for visualization, deployment, distributed computing, etc.\n",
    "\n",
    "We will use _TensorFlow 2.0_ in our labs.\n",
    "\n",
    "### Tensors\n",
    "\n",
    "`Tensor` is the basic TF type. Constants called by `tf.constant` are immutable, while variables called by `tf.Variable` can be changed later. Therefore variables are used as model parameters. Notice that each tensor has a `shape` and a `dtype`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.arange(15, dtype=np.float).reshape(5,3)\n",
    "print(np_array)\n",
    "\n",
    "tf_tensor = tf.constant(np_array)\n",
    "print(tf_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the usual matrix operations with these tensors, e.g. $\\sigma(\\mathbf{Wx} + \\mathbf{b})$. Common operators, such as `+` for addition or `@` for matrix multiplication are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "w = tf.constant([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.2, 0.1, 0.3],\n",
    "    [0.3, 0.1, 0.2]\n",
    "])\n",
    "\n",
    "x = tf.constant([\n",
    "    [0.5],\n",
    "    [-0.3],\n",
    "    [0.2],\n",
    "])\n",
    "\n",
    "b = tf.constant([\n",
    "    [0.3],\n",
    "    [-0.4],\n",
    "    [0.2],\n",
    "])\n",
    "\n",
    "print(sigma(w @ x + b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we need to define $\\mathbf{x}$ and $\\mathbf{b}$ as 2D tensors with shape `(3, 1)` if we want them to behave like column vectors.\n",
    "\n",
    "### Defining models\n",
    "\n",
    "We will use a high-level API for model definition called `keras`. Within this API we have `tf.keras.Model` class for models. Models are a basic unit that transforms input $x$ to output $\\hat{y}$ and that can be trained via SGD or other similar algorithm. \n",
    "\n",
    "We will define it using predefined `layers`. Layers are atomic units of computation, that can be reused, e.g. `Dense` is an implementation of MLP layer equation: $\\sigma(\\mathbf{Wx} + \\mathbf{b})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(keras.Model):  # Subclassing\n",
    "    \n",
    "    def __init__(self, dim_output, dim_hidden):  # init is used to initialize the layers we will use\n",
    "        super(MultilayerPerceptron, self).__init__(name='multilayer_perceptron')\n",
    "        self.dim_output = dim_output\n",
    "        self.dim_hidden = dim_hidden\n",
    "\n",
    "        self.layer_1 = keras.layers.Dense(\n",
    "            units=dim_hidden)  # units = how many neurons in the layer\n",
    "        self.layer_2 = keras.layers.Dense(\n",
    "            units=dim_output)\n",
    "\n",
    "    def call(self, x):  # call defines the flow of the computation, e.g. in this particular model\n",
    "                        # we simply call the two layers one after the oter\n",
    "        h = self.layer_1(x)\n",
    "        y = self.layer_2(h)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 4.1:__ Check the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) of `Dense` layer. The arguments of the layers provide us with multiple options, you should know at least the first five arguments. What is missing in the definition used above?\n",
    "\n",
    "### Training models\n",
    "\n",
    "We will try this model to classify the _Iris_ dataset from previous lab. Training models defined like this is really easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data('iris.csv', num_classes=3)\n",
    "\n",
    "# Reminder how these data look\n",
    "for x, y in list(zip(data.x, data.y))[:5]:  # First 5 samples\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(  # We create new mdoel\n",
    "    dim_output=3,\n",
    "    dim_hidden=32)\n",
    "\n",
    "model.compile(  # By compiling we prepare the model for training\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003),  # We pick a optimizer algorithm\n",
    "    loss='mean_squared_error',  # We pick a loss function\n",
    "    metrics=['accuracy'])  # We pick evaluation metrics\n",
    "\n",
    "model.fit(  # Fit runs the training over provided data\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the selling point for using modern neural frameworks. The model is trained via SGD, but we do not need to calculate derivatives. Instead they are calculated automatically by TF. We also do not need to program how SGD works, nor we need to define the loss funcions or metrics.\n",
    "\n",
    "All that we done manually last week is now hidden behind the `fit` function. You should already be familiar with all the concepts that were introduced in the code below, such as `epochs`, `batch_size`, `metrics`, `loss`, `optimizer`, etc.\n",
    "\n",
    "### Programming assignment 4.2: Multilayer Perceptron [1pt]\n",
    "\n",
    "Extend the `MultilayerPerceptron` definition above so that we can have model with arbitrary number of layer and arbitrary activation functions. Check the call below to see how it should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32,\n",
    "    num_layers=3,\n",
    "    activation=keras.activations.sigmoid)\n",
    "\n",
    "# compile and fit are the same as above\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks\n",
    "\n",
    "Apart from automatic training, TF also provides us with a lot of pre-programmed parts, such as:\n",
    "\n",
    "- [Loss functions](https://www.tensorflow.org/api_docs/python/tf/keras/losses), such as mean squared error.\n",
    "- [Activation functions](https://www.tensorflow.org/api_docs/python/tf/keras/activations), such as sigmoid or ReLU.\n",
    "- [Optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), such as SGD.\n",
    "- [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics), such as accuracy.\n",
    "- [Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers), such as Dense layer, which is basically the MLP layer $\\sigma(\\mathbf{Wx} + \\mathbf{b})$.\n",
    "- [Initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers), such as Glorot (Xavier) initialization.\n",
    "- and many other features.\n",
    "\n",
    "You have seen an example of each of these parts in the code above. E.g. we can use `loss='mean_squared_error'` because a loss function with such name is defiend in `keras.losses`. You should be able to program most of your projects with these pre-programmed building blocks. But you can of course define your own blocks by following the documentation.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "`fit` also has support for creating test set, called `validation set` in TF. By using `validation_split` in `fit` it uses part of the data for evaluation. This is the same concept we have done last week. Check the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) of `fit` to see what other options it has - you should understand most of them. Run the following code to see how it looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32,\n",
    "    num_layers=3,\n",
    "    activation=keras.activations.sigmoid)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20,\n",
    "    validation_split=0.2)  # This was added\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "_TensorBoard_ (TB) is a great visualization tool for training TF models. First, we need to tell the model that it should create TB-related logs during training. The easiest way is to use callbacks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32,\n",
    "    num_layers=3,\n",
    "    activation=keras.activations.sigmoid)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.003),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=os.path.join(\"logs\", timestamp()),\n",
    "    histogram_freq=1)\n",
    "\n",
    "model.fit(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    batch_size=4,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[tensorboard_callback],  # Callback\n",
    "    verbose=0)  # Supressing text output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visulize the results with TensorBoard. We can start it from terminal, or directly from jupyter (run the next cell). After you start TB, you can access it from the notebook, or more conveniently directly from your browser at http://localhost:6006.\n",
    "\n",
    "The first two tabs _Scalars_ and _Graphs_ are the most interesting your you right now. The first shows how do various quantities change during the epochs. It shows them for both train and validation data. You can also see the results for multiple runs at the same time. Run the training above again, but change some hyperparameters, e.g. learning rate. Then you can directly compare the results in TB.\n",
    "\n",
    "_Graphs_ show a graph of your model, i.e. how does it compute the results. By double-clicking you can open individual parts and see how are they defined, i.e. open your model and then a dense layer within to see how is it defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming assignment 4.3: Multilayer Perceptron, Part 2 [1pt]\n",
    "\n",
    "First, run the current model with TB callback to get some results. Then implement the following changes:\n",
    "\n",
    "1. Use _softmax_ as an activation function for the __last__ layer.\n",
    "2. Use _categorical crossentropy_ as a loss function\n",
    "\n",
    "You can find both these functions in the links of pre-programmed building blocks above. Then compare the results of this new implementation in TensorBoard.\n",
    "\n",
    "#### Submission\n",
    "\n",
    "Save the code for your `MultilayerPerceptron` and you training commands in a `mlp.py` file and submit them to AIS. You need to complete __PA 4.2__ before proceeding to __PA 4.3__. There are no tests this week so consult the submission with your teacher if needed.\n",
    "\n",
    "## Gradient Tape\n",
    "\n",
    "`fit` is a very convenient way of training neural models, but sometimes we need more flexibility and control. For example, with `fit` we can not track the training step by step (e.g. for debugging). The model is compiled into a computation graph in the background. So if you want to have a debugging print within a model, it will not run. E.g., try printing the value of `h` in the model `call`.\n",
    "\n",
    "Instead we can use so called `GradientType`. With this tape the debugging print of `h` will run. Check the following code, it is very similar in how we defined SGD in previous labs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    dim_output=3,\n",
    "    dim_hidden=32)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "\n",
    "# loss_function = keras.losses.CategoricalCrossentropy()\n",
    "# You can use cross-entropy loss if you completed PA 4.3\n",
    "    \n",
    "def step(xs, ys):  # This has the same meaning as step function in previous labs\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(xs)  # Model predictions\n",
    "        loss = loss_function(ys, preds)  # The value of loss function comparing the true\n",
    "                                         # values ys with predictions\n",
    "\n",
    "    gradient = tape.gradient(\n",
    "        target=loss,\n",
    "        sources=model.trainable_variables)  # Calculate the gradient of loss function w.r.t. model parameters.\n",
    "                                            # This behaves the same as gradient methods from previous labs.\n",
    "        \n",
    "    optimizer.apply_gradients(zip(gradient, model.trainable_variables))  # Applies the computed gradient on current\n",
    "                                                                         # parameter values.\n",
    "    \n",
    "def loss(xs, ys):\n",
    "    preds = model(xs)\n",
    "    return loss_function(ys, preds)\n",
    "    \n",
    "num_epochs = 100\n",
    "batch_size = 5\n",
    "num_samples = len(data.x)\n",
    "\n",
    "# Training loop\n",
    "for e in range(num_epochs):\n",
    "    for i in np.arange(0, num_samples, batch_size):  # Batching\n",
    "        step(data.x[i:i+batch_size], data.y[i:i+batch_size])\n",
    "    print('Epoch:', e, 'Loss:', loss(data.x, data.y).numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "Check out TF [Tutorials](https://www.tensorflow.org/tutorials) and [Guide](https://www.tensorflow.org/guide) for some further reading. Note that all the documents there are in fact Jupyter notebooks, so you can download them and run them here. At this point, you should check:\n",
    "\n",
    "- _Tutorials > ML basics with Keras_ - for some basic practical use cases.\n",
    "\n",
    "And you can also check:\n",
    "\n",
    "- _Guide > Keras_ - for more in-depth explanation of how TF works.\n",
    "\n",
    "You can also check this [notebook](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO) showcasing and explaining additional features.\n",
    "\n",
    "## Correct Answers\n",
    "\n",
    "__E 4.1:__ Activation function is missing. `Dense` uses linear activation by default. We need to use the `activation` argument to add an activation function. There we can use our own activation functions, such as `sigma` defined previously. Or, we can use some of the pre-programmed activation functions from `tf.keras.activations` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
